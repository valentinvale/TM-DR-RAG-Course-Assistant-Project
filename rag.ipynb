{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0498db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DIR = \"data/aml_raw\"\n",
    "META_FILE = os.path.join(RAW_DIR, \"metadata.jsonl\")\n",
    "OUT_FILE = \"data/aml_chunks.jsonl\"\n",
    "\n",
    "MIN_CHARS = 800 # minimum number of characters per chunk\n",
    "MAX_CHARS = 1400 # maximum number of characters per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load metadata\n",
    "def load_metadata(path):\n",
    "    meta = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            meta.append(json.loads(line))\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee44d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from PDF pages\n",
    "def extract_pages(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        if text:\n",
    "            pages.append((i + 1, text))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d8d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine if a split should occur\n",
    "def should_split(text):\n",
    "    triggers = [\n",
    "        \"Definition\", \"Theorem\", \"Lemma\",\n",
    "        \"Proof\", \"Algorithm\", \"Recap\",\n",
    "        \"Today‚Äôs lecture\", \"Overview\"\n",
    "    ]\n",
    "    return any(t in text for t in triggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5a9432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:00<00:00, 32.94it/s]\n"
     ]
    }
   ],
   "source": [
    "metadata = load_metadata(META_FILE)\n",
    "\n",
    "with open(OUT_FILE, \"w\", encoding=\"utf-8\") as out:\n",
    "    for doc_meta in tqdm(metadata, desc=\"Chunking PDFs\"):\n",
    "        pdf_path = doc_meta[\"source_file\"]\n",
    "        if not os.path.exists(pdf_path):\n",
    "            continue\n",
    "\n",
    "        pages = extract_pages(pdf_path)\n",
    "\n",
    "        buffer = \"\"\n",
    "        page_start = None\n",
    "        chunk_id = 0\n",
    "\n",
    "        for page_num, text in pages:\n",
    "\n",
    "            if page_num <= 5:\n",
    "                low = text.lower()\n",
    "                if (\"administrative\" in low\n",
    "                    or \"seminar\" in low and \"room\" in low\n",
    "                    or \"uploaded\" in low\n",
    "                    or \"attendance\" in low\n",
    "                    or \"exam\" in low and \"date\" in low):\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                \n",
    "\n",
    "            if page_start is None:\n",
    "                page_start = page_num\n",
    "\n",
    "            if should_split(text) and len(buffer) >= MIN_CHARS:\n",
    "                chunk = {\n",
    "                    \"chunk_id\": f\"{doc_meta['doc_id']}_p{page_start}_c{chunk_id}\",\n",
    "                    \"doc_id\": doc_meta[\"doc_id\"],\n",
    "                    \"type\": doc_meta[\"type\"],\n",
    "                    \"index\": doc_meta[\"index\"],\n",
    "                    \"page_start\": page_start,\n",
    "                    \"page_end\": page_num - 1,\n",
    "                    \"source_file\": os.path.basename(pdf_path),\n",
    "                    \"text\": buffer.strip()\n",
    "                }\n",
    "                out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "                chunk_id += 1\n",
    "                buffer = \"\"\n",
    "                page_start = page_num\n",
    "\n",
    "            buffer += \"\\n\" + text\n",
    "\n",
    "            if len(buffer) >= MAX_CHARS:\n",
    "                chunk = {\n",
    "                    \"chunk_id\": f\"{doc_meta['doc_id']}_p{page_start}_c{chunk_id}\",\n",
    "                    \"doc_id\": doc_meta[\"doc_id\"],\n",
    "                    \"type\": doc_meta[\"type\"],\n",
    "                    \"index\": doc_meta[\"index\"],\n",
    "                    \"page_start\": page_start,\n",
    "                    \"page_end\": page_num,\n",
    "                    \"source_file\": os.path.basename(pdf_path),\n",
    "                    \"text\": buffer.strip()\n",
    "                }\n",
    "                out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "                chunk_id += 1\n",
    "                buffer = \"\"\n",
    "                page_start = None\n",
    "\n",
    "        if buffer.strip():\n",
    "            chunk = {\n",
    "                \"chunk_id\": f\"{doc_meta['doc_id']}_p{page_start}_c{chunk_id}\",\n",
    "                \"doc_id\": doc_meta[\"doc_id\"],\n",
    "                \"type\": doc_meta[\"type\"],\n",
    "                \"index\": doc_meta[\"index\"],\n",
    "                \"page_start\": page_start,\n",
    "                \"page_end\": pages[-1][0],\n",
    "                \"source_file\": os.path.basename(pdf_path),\n",
    "                \"text\": buffer.strip()\n",
    "            }\n",
    "            out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acea0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "IN_FILE = \"data/aml_chunks.jsonl\"\n",
    "OUT_FILE = \"data/aml_chunks.cleaned.jsonl\"\n",
    "\n",
    "MIN_LEN = 80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b2a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                items.append(json.loads(line))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab0487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path, items):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in items:\n",
    "            f.write(json.dumps(it, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4639e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_into_next(chunks, min_len=80):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        cur = chunks[i]\n",
    "        cur_text = (cur.get(\"text\") or \"\").strip()\n",
    "\n",
    "        if len(cur_text) < min_len:\n",
    "            # try merge into next chunk if same doc_id\n",
    "            if i + 1 < len(chunks) and chunks[i + 1].get(\"doc_id\") == cur.get(\"doc_id\"):\n",
    "                nxt = chunks[i + 1]\n",
    "                nxt_text = (nxt.get(\"text\") or \"\").strip()\n",
    "\n",
    "                # prepend tiny text to next chunk\n",
    "                combined = (cur_text + \"\\n\" + nxt_text).strip()\n",
    "                nxt[\"text\"] = combined\n",
    "\n",
    "                # expand page range to include the tiny chunk\n",
    "                nxt[\"page_start\"] = min(cur.get(\"page_start\", nxt.get(\"page_start\")), nxt.get(\"page_start\"))\n",
    "                # page_end stays nxt's page_end (it already ends later)\n",
    "\n",
    "                # you can optionally record merged-from ids\n",
    "                merged_from = nxt.get(\"merged_from\", [])\n",
    "                merged_from.append(cur.get(\"chunk_id\"))\n",
    "                nxt[\"merged_from\"] = merged_from\n",
    "\n",
    "                # skip cur (drop it) and move to next\n",
    "                i += 1\n",
    "            else:\n",
    "                # can't merge forward ‚Üí try merge backward into previous if same doc_id\n",
    "                if out and out[-1].get(\"doc_id\") == cur.get(\"doc_id\"):\n",
    "                    prev = out[-1]\n",
    "                    prev[\"text\"] = (prev.get(\"text\",\"\").strip() + \"\\n\" + cur_text).strip()\n",
    "                    prev[\"page_end\"] = max(prev.get(\"page_end\", cur.get(\"page_end\")), cur.get(\"page_end\"))\n",
    "\n",
    "                    merged_from = prev.get(\"merged_from\", [])\n",
    "                    merged_from.append(cur.get(\"chunk_id\"))\n",
    "                    prev[\"merged_from\"] = merged_from\n",
    "                # else: drop it silently (or keep it if you prefer)\n",
    "        else:\n",
    "            out.append(cur)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6082fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned chunks written to data/aml_chunks.cleaned.jsonl, from 165 to 164 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = read_jsonl(IN_FILE)\n",
    "cleaned_chunks = merge_small_into_next(chunks, min_len=MIN_LEN)\n",
    "write_jsonl(OUT_FILE, cleaned_chunks)\n",
    "print(f\"Cleaned chunks written to {OUT_FILE}, from {len(chunks)} to {len(cleaned_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb54a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept: 157 dropped: 7 -> data/aml_chunks.final.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "\n",
    "IN_FILE = \"data/aml_chunks.cleaned.jsonl\"\n",
    "OUT_FILE = \"data/aml_chunks.final.jsonl\"\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    r\"\\badministrative\\b\",\n",
    "    r\"\\bexam date\\b\",\n",
    "    r\"\\blecture\\b\\s*:\\s*\",\n",
    "    r\"\\bseminar\\b\\s*:\\s*\",\n",
    "    r\"\\broom\\b\\s*\\d+\",\n",
    "    r\"\\bschedule\\b\",\n",
    "]\n",
    "\n",
    "drop_re = re.compile(\"|\".join(DROP_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "kept = 0\n",
    "dropped = 0\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding=\"utf-8\") as fin, open(OUT_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        text = (obj.get(\"text\") or \"\").strip()\n",
    "\n",
    "        # Only drop if it looks like logistics (early pages), to avoid false positives\n",
    "        is_early = (obj.get(\"page_end\", 9999) <= 12)\n",
    "        if is_early and drop_re.search(text):\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        kept += 1\n",
    "\n",
    "print(\"kept:\", kept, \"dropped:\", dropped, \"->\", OUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b58d1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss  # assuming this now works on your system\n",
    "\n",
    "DATA_FILE = \"data/aml_chunks.final.jsonl\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b46696ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"what\", \"is\", \"the\", \"a\", \"an\", \"in\", \"on\", \"of\", \"to\", \"and\", \"or\", \"for\", \"with\",\n",
    "    \"as\", \"this\", \"that\", \"it\", \"be\", \"are\", \"was\", \"were\", \"do\", \"does\", \"did\",\n",
    "    \"about\", \"behind\", \"explain\", \"define\", \"idea\", \"main\"\n",
    "}\n",
    "\n",
    "_word_re = re.compile(r\"[A-Za-z0-9_]+\")\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    tokens = [t.lower() for t in _word_re.findall(text)]\n",
    "    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "\n",
    "\n",
    "def load_chunks(path: str) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                chunks.append(json.loads(line))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58580331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    score: float\n",
    "    chunk: Dict[str, Any]\n",
    "    retriever: str   # üëà NEW FIELD\n",
    "\n",
    "    def cite(self) -> str:\n",
    "        src = self.chunk.get(\"source_file\", \"\")\n",
    "        ps = self.chunk.get(\"page_start\", \"\")\n",
    "        pe = self.chunk.get(\"page_end\", \"\")\n",
    "        return f\"{src} p.{ps}\" if ps == pe else f\"{src} p.{ps}-{pe}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f476ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "        self.tokens = [tokenize(c.get(\"text\",\"\")) for c in chunks]\n",
    "        self.bm25 = BM25Okapi(self.tokens)\n",
    "\n",
    "    def search(self, query, k=5, type_filter=\"lecture\"):\n",
    "        scores = self.bm25.get_scores(tokenize(query))\n",
    "        idxs = np.argsort(scores)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in idxs:\n",
    "            c = self.chunks[int(i)]\n",
    "            if type_filter and c.get(\"type\") != type_filter:\n",
    "                continue\n",
    "            results.append(\n",
    "                SearchResult(float(scores[int(i)]), c, \"bm25\")\n",
    "            )\n",
    "            if len(results) >= k:\n",
    "                break\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab797b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingRetriever:\n",
    "    def __init__(self, chunks, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.chunks = chunks\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        texts = [c.get(\"text\",\"\") for c in chunks]\n",
    "        embs = []\n",
    "        for i in tqdm(range(0, len(texts), 64), desc=\"Embedding chunks\"):\n",
    "            batch = texts[i:i+64]\n",
    "            vecs = self.model.encode(batch, normalize_embeddings=True)\n",
    "            embs.append(vecs)\n",
    "\n",
    "        self.embs = np.vstack(embs).astype(\"float32\")\n",
    "\n",
    "    def search(self, query, k=5, type_filter=\"lecture\"):\n",
    "        q = self.model.encode([query], normalize_embeddings=True)[0]\n",
    "        scores = self.embs @ q\n",
    "\n",
    "        idxs = np.argsort(scores)[::-1]\n",
    "        results = []\n",
    "        for i in idxs:\n",
    "            c = self.chunks[int(i)]\n",
    "            if type_filter and c.get(\"type\") != type_filter:\n",
    "                continue\n",
    "            results.append(\n",
    "                SearchResult(float(scores[int(i)]), c, \"emb\")\n",
    "            )\n",
    "            if len(results) >= k:\n",
    "                break\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53118fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 157 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Embedding chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = load_chunks(DATA_FILE)\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "bm25 = BM25Retriever(chunks)\n",
    "emb = EmbeddingRetriever(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "811729d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY: What is generalization error in statistical learning?\n",
      "\n",
      "=== BM25 ===\n",
      "- score=8.9361 | aml_lecture_03 | Lecture03.pdf p.1-3\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 Recap ‚Ä¢ A Formal Model ‚Äì The Statistical learning framework ‚Äì papaya tasting learning scenario, classification task: tasty ...\n",
      "- score=6.8099 | aml_lecture_01 | Lecture01.pdf p.56-61\n",
      "  ‚Ä¢ Survey of prominent methods and approaches with strong  theoretical foundations such as: ‚Äì Boosting ‚Äì SVMs ‚Äì neural networks? (loose bounds, work in progress) ‚Äì etc Course Structure ‚Äì Part 2 Usefulness of Theoretical  Machine Learning Per...\n",
      "- score=6.0135 | aml_lecture_08 | Lecture08.pdf p.1-6\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 The fundamental theorem of  statistical learning The fundamental theorem of statistical learning Theorem (The Fundamental T...\n",
      "- score=5.5245 | aml_lecture_12 | Lecture12.pdf p.20-25\n",
      "  Generalization error for AdaBoost Lemma Let B be a base class and let L(B,T ) be as defined as previously: L(B,T) = {h: Rd‚Üí{-1,1},                , w ‚àà RT, ht ‚àà B} Assume that both T and VCdim(B) are at least 3. Then, we have that: VCdim(L(...\n",
      "- score=5.4920 | aml_lecture_11 | Lecture11.pdf p.13-22\n",
      "  Toy example Toy example Toy example Toy example Toy example ‚Äì final classifier Next week: Viola-Jones face detector Main idea: ‚Äì represent local texture with efficiently computable  ‚Äúrectangular‚Äù features within window of interest ‚Äì select ...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.6118 | aml_lecture_02 | Lecture02.pdf p.8-11\n",
      "  ‚ÄúCorrect on future examples‚Äù ‚Ä¢ let f be the correct classifier, then we should find h such that h ‚âà f ‚Ä¢ one way: define the error of h w.r.t. f to be the probability that it does not predict  the correct label on a random data point x gener...\n",
      "- score=0.5332 | aml_lecture_12 | Lecture12.pdf p.18-19\n",
      "  Generalization error for AdaBoost A popular approach for constructing a weak learner is to apply the ERM  rule with respect to a base hypothesis class B (e.g., ERM over HDSd =  decision stumps in d dimensions).  Given a base hypothesis clas...\n",
      "- score=0.5020 | aml_lecture_01 | Lecture01.pdf p.56-61\n",
      "  ‚Ä¢ Survey of prominent methods and approaches with strong  theoretical foundations such as: ‚Äì Boosting ‚Äì SVMs ‚Äì neural networks? (loose bounds, work in progress) ‚Äì etc Course Structure ‚Äì Part 2 Usefulness of Theoretical  Machine Learning Per...\n",
      "- score=0.5001 | aml_lecture_04 | Lecture04.pdf p.17-19\n",
      "  Uniform Convergence ‚Ä¢ given H, the ERMH learning paradigm works as follows: ‚Äì based on a received training sample S of examples draw i.i.d from an unknown  distribution D over a domain Z = X √ó Y, ERMH evaluates the risk (error) of  each h i...\n",
      "- score=0.4959 | aml_lecture_08 | Lecture08.pdf p.1-6\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 The fundamental theorem of  statistical learning The fundamental theorem of statistical learning Theorem (The Fundamental T...\n",
      "\n",
      "QUERY: Explain gamma-weak learnability\n",
      "\n",
      "=== BM25 ===\n",
      "- score=7.7008 | aml_lecture_11 | Lecture11.pdf p.23-24\n",
      "  Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a  function mH :(0,1)‚ÜíN such that: ‚Ä¢ for every Œ¥ > 0                   (confidence) ‚Ä¢ for every labeling f ‚àà H, f ...\n",
      "- score=7.4510 | aml_lecture_12 | Lecture12.pdf p.3-5\n",
      "  Recap - Weak learnability - example Let X = R, H is the class of 3-piece classifiers (signed intervals):  H = {hŒ∏1,Œ∏2,b Œ∏1,Œ∏2 ‚ààR,Œ∏1 <Œ∏2,b ‚àà{‚àí1,+1}} hŒ∏1,Œ∏2,b(x) = +b, if x < Œ∏1 or x > Œ∏2  -b, if Œ∏1 ‚â§ x ‚â§ Œ∏2  Consider B the class of Decision ...\n",
      "- score=7.3364 | aml_lecture_11 | Lecture11.pdf p.25-26\n",
      "  Weak vs Strong learnability The fundamental theorem of learning (lecture 8) states that if a hypothesis class H has a VC dimension d, then the sample complexity of PAC learning  H satisfies Applying this with  Œµ = 1/2‚àíŒ≥ we immediately obtai...\n",
      "- score=6.9446 | aml_lecture_12 | Lecture12.pdf p.1-2\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 Recap - Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if th...\n",
      "- score=6.0494 | aml_lecture_11 | Lecture11.pdf p.27-29\n",
      "  Weak learnability - example ERMB is a Œ≥-weak learner for H, for Œ≥ = 1/12. Proof: Consider a h* =          ‚àà H that labels a training set S. Œ∏1 * Consider a distribution D over X = R. Then, we are sure that at least one of  the regions (-‚àû, ...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.7643 | aml_lecture_12 | Lecture12.pdf p.1-2\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 Recap - Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if th...\n",
      "- score=0.7518 | aml_lecture_11 | Lecture11.pdf p.23-24\n",
      "  Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a  function mH :(0,1)‚ÜíN such that: ‚Ä¢ for every Œ¥ > 0                   (confidence) ‚Ä¢ for every labeling f ‚àà H, f ...\n",
      "- score=0.6219 | aml_lecture_11 | Lecture11.pdf p.25-26\n",
      "  Weak vs Strong learnability The fundamental theorem of learning (lecture 8) states that if a hypothesis class H has a VC dimension d, then the sample complexity of PAC learning  H satisfies Applying this with  Œµ = 1/2‚àíŒ≥ we immediately obtai...\n",
      "- score=0.5537 | aml_lecture_11 | Lecture11.pdf p.27-29\n",
      "  Weak learnability - example ERMB is a Œ≥-weak learner for H, for Œ≥ = 1/12. Proof: Consider a h* =          ‚àà H that labels a training set S. Œ∏1 * Consider a distribution D over X = R. Then, we are sure that at least one of  the regions (-‚àû, ...\n",
      "- score=0.5468 | aml_lecture_12 | Lecture12.pdf p.3-5\n",
      "  Recap - Weak learnability - example Let X = R, H is the class of 3-piece classifiers (signed intervals):  H = {hŒ∏1,Œ∏2,b Œ∏1,Œ∏2 ‚ààR,Œ∏1 <Œ∏2,b ‚àà{‚àí1,+1}} hŒ∏1,Œ∏2,b(x) = +b, if x < Œ∏1 or x > Œ∏2  -b, if Œ∏1 ‚â§ x ‚â§ Œ∏2  Consider B the class of Decision ...\n",
      "\n",
      "QUERY: What is the idea behind AdaBoost?\n",
      "\n",
      "=== BM25 ===\n",
      "- score=4.1546 | aml_lecture_12 | Lecture12.pdf p.20-25\n",
      "  Generalization error for AdaBoost Lemma Let B be a base class and let L(B,T ) be as defined as previously: L(B,T) = {h: Rd‚Üí{-1,1},                , w ‚àà RT, ht ‚àà B} Assume that both T and VCdim(B) are at least 3. Then, we have that: VCdim(L(...\n",
      "- score=4.0724 | aml_lecture_12 | Lecture12.pdf p.18-19\n",
      "  Generalization error for AdaBoost A popular approach for constructing a weak learner is to apply the ERM  rule with respect to a base hypothesis class B (e.g., ERM over HDSd =  decision stumps in d dimensions).  Given a base hypothesis clas...\n",
      "- score=3.4943 | aml_lecture_12 | Lecture12.pdf p.11-17\n",
      "  Toy example D(1) Weak classifiers = vertical or horizontal half- planes = hypothesis from HDS 2 Toy example ‚Äì Round 1 D(1) D(2) Œµ1 = 0.30 w1 = 0.42 Weak classifiers = vertical or horizontal half- planes = hypothesis from HDS 2 Toy example ‚Äì...\n",
      "- score=3.2513 | aml_lecture_11 | Lecture11.pdf p.34-35\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "- score=3.2240 | aml_lecture_12 | Lecture12.pdf p.9-10\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.4530 | aml_lecture_01 | Lecture01.pdf p.62-64\n",
      "  AdaBoost Algorithm Start with  uniform weights  on training  examples Evaluate weighted  error for each  feature, pick best. Re-weight the examples: Incorrectly classified -> more weight Correctly classified -> less weight Final classifier ...\n",
      "- score=0.3607 | aml_lecture_12 | Lecture12.pdf p.18-19\n",
      "  Generalization error for AdaBoost A popular approach for constructing a weak learner is to apply the ERM  rule with respect to a base hypothesis class B (e.g., ERM over HDSd =  decision stumps in d dimensions).  Given a base hypothesis clas...\n",
      "- score=0.3340 | aml_lecture_11 | Lecture11.pdf p.36-41\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "- score=0.3158 | aml_lecture_12 | Lecture12.pdf p.40-45\n",
      "  AdaBoost Algorithm Start with  uniform weights  on training  examples Evaluate weighted  error for each  feature, pick best. Re-weight the examples: Incorrectly classified -> more weight Correctly classified -> less weight Final classifier ...\n",
      "- score=0.3038 | aml_lecture_11 | Lecture11.pdf p.34-35\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "\n",
      "QUERY: What does hard-margin SVM optimize?\n",
      "\n",
      "=== BM25 ===\n",
      "- score=20.2331 | aml_lecture_13 | Lecture13.pdf p.20-25\n",
      "  Hard SVM Hard-SVM is the learning rule in which we return an ERM hyperplane  that separates the training set  S = {(x1, y1), (x1, y1), ‚Ä¶, (xm, ym)} with the  largest possible margin.    maximize subject to   Œ≥ = 1 w Œ≥ y(i) x(i), w w + b w ‚éõ...\n",
      "- score=17.9671 | aml_lecture_13 | Lecture13.pdf p.16-19\n",
      "  https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf Sample complexity for separability case ‚Ä¢ if the number of examples is significantly smaller than d/Œµ then no  algorithm can learn an accurate halfspace.  ‚Äì this is problema...\n",
      "- score=17.9224 | aml_lecture_13 | Lecture13.pdf p.26-28\n",
      "  Sample complexity for separability case If S = {(x1,y1), (x2,y2),‚Ä¶, (xm,ym)} separable with margin Œ≥ then we have that  S‚Äô = {(Œ±x1,y1), (Œ±x2,y2),‚Ä¶, (Œ±xm,ym)} is separable with margin Œ±Œ≥ for any Œ± > 0.   It can be shown that the sample compl...\n",
      "- score=16.8881 | aml_lecture_13 | Lecture13.pdf p.29-33\n",
      "  Soft SVM ‚Ä¢ this can be modelled by introducing nonnegative slack variables ùùÉ1,  ùùÉ2,‚Ä¶, ùùÉm, and replacing each constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 by  the constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 ‚àíùúâ! That is,  ùùÉi measures by  how much the constraint ùë¶! < ùë§, ùë•! ...\n",
      "- score=4.8534 | aml_lecture_11 | Lecture11.pdf p.6-8\n",
      "  Improper learning Hardness of learning Some classes are hard to PAC learn if we place certain restrictions on the hypothesis  class used by the learning algorithm ‚Ä¢ the problem of properly learning 3-term DNF formulae is computationally har...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.6913 | aml_lecture_13 | Lecture13.pdf p.20-25\n",
      "  Hard SVM Hard-SVM is the learning rule in which we return an ERM hyperplane  that separates the training set  S = {(x1, y1), (x1, y1), ‚Ä¶, (xm, ym)} with the  largest possible margin.    maximize subject to   Œ≥ = 1 w Œ≥ y(i) x(i), w w + b w ‚éõ...\n",
      "- score=0.6591 | aml_lecture_13 | Lecture13.pdf p.26-28\n",
      "  Sample complexity for separability case If S = {(x1,y1), (x2,y2),‚Ä¶, (xm,ym)} separable with margin Œ≥ then we have that  S‚Äô = {(Œ±x1,y1), (Œ±x2,y2),‚Ä¶, (Œ±xm,ym)} is separable with margin Œ±Œ≥ for any Œ± > 0.   It can be shown that the sample compl...\n",
      "- score=0.6573 | aml_lecture_13 | Lecture13.pdf p.29-33\n",
      "  Soft SVM ‚Ä¢ this can be modelled by introducing nonnegative slack variables ùùÉ1,  ùùÉ2,‚Ä¶, ùùÉm, and replacing each constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 by  the constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 ‚àíùúâ! That is,  ùùÉi measures by  how much the constraint ùë¶! < ùë§, ùë•! ...\n",
      "- score=0.5352 | aml_lecture_13 | Lecture13.pdf p.16-19\n",
      "  https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf Sample complexity for separability case ‚Ä¢ if the number of examples is significantly smaller than d/Œµ then no  algorithm can learn an accurate halfspace.  ‚Äì this is problema...\n",
      "- score=0.4721 | aml_lecture_13 | Lecture13.pdf p.8-15\n",
      "  Support Vector Machines  (SVMs) SVMs Slide credit Sparktech SVMs Slide credit Sparktech SVMs Slide credit Sparktech SVMs Slide credit Sparktech Recap: The fundamental theorem of statistical learning  ‚Äì quantitative version Theorem Let H be ...\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(label, results, max_chars=240):\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    for r in results:\n",
    "        text = r.chunk[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        print(f\"- score={r.score:.4f} | {r.chunk['doc_id']} | {r.cite()}\")\n",
    "        print(f\"  {text[:max_chars]}{'...' if len(text) > max_chars else ''}\")\n",
    "\n",
    "queries = [\n",
    "    \"What is generalization error in statistical learning?\",\n",
    "    \"Explain gamma-weak learnability\",\n",
    "    \"What is the idea behind AdaBoost?\",\n",
    "    \"What does hard-margin SVM optimize?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQUERY: {q}\")\n",
    "    pretty_print(\"BM25\", bm25.search(q))\n",
    "    pretty_print(\"Embeddings\", emb.search(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf64cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(\n",
    "    bm25_res: List[SearchResult],\n",
    "    emb_res: List[SearchResult],\n",
    "    k: int = 5,\n",
    "    rrf_k: int = 60\n",
    ") -> List[SearchResult]:\n",
    "    # key by chunk_id so identical chunks merge\n",
    "    def key(r: SearchResult) -> str:\n",
    "        return r.chunk[\"chunk_id\"]\n",
    "\n",
    "    scores = {}\n",
    "    best_obj = {}\n",
    "\n",
    "    for rank, r in enumerate(bm25_res, start=1):\n",
    "        kid = key(r)\n",
    "        scores[kid] = scores.get(kid, 0.0) + 1.0 / (rrf_k + rank)\n",
    "        best_obj[kid] = r\n",
    "\n",
    "    for rank, r in enumerate(emb_res, start=1):\n",
    "        kid = key(r)\n",
    "        scores[kid] = scores.get(kid, 0.0) + 1.0 / (rrf_k + rank)\n",
    "        # keep one representative\n",
    "        best_obj.setdefault(kid, r)\n",
    "\n",
    "    fused = []\n",
    "    for kid, sc in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        r = best_obj[kid]\n",
    "        fused.append(SearchResult(score=float(sc), chunk=r.chunk, retriever=\"hybrid\"))\n",
    "        if len(fused) >= k:\n",
    "            break\n",
    "    return fused\n",
    "\n",
    "def retrieve(query: str, mode: str = \"hybrid\", k: int = 5, type_filter: str = \"lecture\") -> List[SearchResult]:\n",
    "    if mode == \"bm25\":\n",
    "        return bm25.search(query, k=k, type_filter=type_filter)\n",
    "    if mode == \"emb\":\n",
    "        return emb.search(query, k=k, type_filter=type_filter)\n",
    "    if mode == \"hybrid\":\n",
    "        b = bm25.search(query, k=max(k, 8), type_filter=type_filter)\n",
    "        e = emb.search(query, k=max(k, 8), type_filter=type_filter)\n",
    "        return rrf_fuse(b, e, k=k)\n",
    "    raise ValueError(\"mode must be one of: bm25, emb, hybrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "252a3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, math\n",
    "import numpy as np\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DATA_FILE = \"data/aml_chunks.final.jsonl\"\n",
    "\n",
    "GROQ_BASE_URL=\"https://api.groq.com/openai/v1/\"\n",
    "GROQ_API_KEY = json.load(open(\"secrets.json\")).get(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dddde2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(results: List[SearchResult], max_chars: int = 6000) -> str:\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        txt = (r.chunk.get(\"text\",\"\") or \"\").strip()\n",
    "        cite = r.cite()\n",
    "        block = f\"[{i}] ({cite})\\n{txt}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a course assistant for Advanced Machine Learning.\n",
    "Rules:\n",
    "- Answer ONLY using the provided context snippets.\n",
    "- If the context does not contain the answer, say: \"Not in the provided course materials.\"\n",
    "- Cite sources inline using the snippet numbers like [1], [2] and include the file+page already shown in those snippets.\n",
    "- Do not invent equations that are not explicitly present in the context. Prefer conceptual explanations.\n",
    "- Be concise and accurate.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35c14bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def groq_chat(\n",
    "    messages,\n",
    "    model=GROQ_MODEL,\n",
    "    temperature=0.2,\n",
    "    max_tokens=400,\n",
    "    max_retries=5\n",
    ") -> str:\n",
    "    url = f\"{GROQ_BASE_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Rate limited. Sleeping {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    raise RuntimeError(\"Groq API failed after retries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a366822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(\n",
    "    query: str,\n",
    "    mode: str = \"hybrid\",      # \"bm25\" | \"emb\" | \"hybrid\"\n",
    "    k: int = 5,\n",
    "    type_filter: str = \"lecture\",\n",
    "    model: str = GROQ_MODEL\n",
    ") -> dict:\n",
    "    results = retrieve(query, mode=mode, k=k, type_filter=type_filter)\n",
    "    context = build_context(results)\n",
    "\n",
    "    user_prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Context snippets:\n",
    "{context}\n",
    "\n",
    "Write the answer now following the rules.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    answer = groq_chat(messages, model=model, temperature=0.2, max_tokens=450)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"mode\": mode,\n",
    "        \"k\": k,\n",
    "        \"type_filter\": type_filter,\n",
    "        \"retrieved\": [{\n",
    "            \"rank\": i+1,\n",
    "            \"retriever\": r.retriever,\n",
    "            \"score\": r.score,\n",
    "            \"chunk_id\": r.chunk[\"chunk_id\"],\n",
    "            \"doc_id\": r.chunk[\"doc_id\"],\n",
    "            \"cite\": r.cite(),\n",
    "        } for i, r in enumerate(results)],\n",
    "        \"answer\": answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "959509a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODE: bm25\n",
      "The generalization error (true risk) in statistical learning is defined as LD,f (h), which is the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is also referred to as the true error of h or the real risk of h. This error is unknown to the learner as they do not know the distribution D and the target function f [1] (Lecture02.pdf p.8-11).\n",
      "\n",
      "================================================================================\n",
      "MODE: emb\n",
      "The generalization error, also known as the true risk, is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and represents the true error of h. The goal is to find a hypothesis h such that LD,f (h) is small, which means that h generalizes well to new, unseen data [1] (Lecture02.pdf p.8-11).\n",
      "\n",
      "================================================================================\n",
      "MODE: hybrid\n",
      "The generalization error (true risk) in statistical learning is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and represents the true error of h, or the real risk of h. This error is unknown to the learner as they do not know the distribution D and the target function f [1] (Lecture02.pdf p.8-11).\n"
     ]
    }
   ],
   "source": [
    "q = \"What is generalization error (true risk) in statistical learning?\"\n",
    "for mode in [\"bm25\", \"emb\", \"hybrid\"]:\n",
    "    out = rag_answer(q, mode=mode, k=5, type_filter=\"lecture\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODE:\", mode)\n",
    "    print(out[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95c8e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_only_answer(query: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the question as best as you can.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    return groq_chat(messages, temperature=0.7, max_tokens=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e922f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "QUESTION: What is generalization error (true risk)?\n",
      "\n",
      "LLM-only:\n",
      " Generalization error, also known as true risk, is a concept in machine learning and statistical learning theory. It refers to the difference between the expected loss (or risk) of a model on unseen, out-of-sample data and the loss on the training data.\n",
      "\n",
      "In other words, generalization error measures how well a model will perform on new, unseen data, as opposed to the data it was trained on. It is called \"generalization\" because it reflects the model's ability to generalize its learning to new situations.\n",
      "\n",
      "The generalization error is typically denoted as:\n",
      "\n",
      "R = E[(f(x) - y)^2]\n",
      "\n",
      "where:\n",
      "- R is the generalization error (or true risk)\n",
      "- E is the expected value\n",
      "- f(x) is the prediction made by the model\n",
      "- y is the true output\n",
      "- x is the input data\n",
      "\n",
      "The generalization error is often contrasted with the empirical risk, which is the average loss on the training data. The empirical risk is used as a proxy for the generalization error, but it is not the same thing. A model that performs well on the training data (low empirical risk) may not necessarily perform well on new, unseen data (high generalization error).\n",
      "\n",
      "Minimizing the generalization error is the ultimate goal of machine learning, as it reflects the model's ability to make accurate predictions on new, unseen data. However, the generalization error is typically unknown, as it depends on the underlying distribution of the data\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "Rate limited. Sleeping 8s...\n",
      "Rate limited. Sleeping 16s...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Groq API failed after retries",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     29\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q,\n\u001b[0;32m     30\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: mode,\n\u001b[0;32m     31\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     32\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved\u001b[39m\u001b[38;5;124m\"\u001b[39m: out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     33\u001b[0m             })\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m---> 37\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 25\u001b[0m, in \u001b[0;36mrun_eval\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLLM-only:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, llm_ans)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbm25\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrag_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlecture\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRAG (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     28\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: q,\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: mode,\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved\u001b[39m\u001b[38;5;124m\"\u001b[39m: out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     33\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m, in \u001b[0;36mrag_answer\u001b[1;34m(query, mode, k, type_filter, model)\u001b[0m\n\u001b[0;32m     11\u001b[0m     user_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;124mContext snippets:\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124mWrite the answer now following the rules.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     17\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[0;32m     19\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[0;32m     20\u001b[0m     ]\n\u001b[1;32m---> 22\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgroq_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m450\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer\n\u001b[0;32m     38\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[23], line 36\u001b[0m, in \u001b[0;36mgroq_chat\u001b[1;34m(messages, model, temperature, max_tokens, max_retries)\u001b[0m\n\u001b[0;32m     33\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroq API failed after retries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Groq API failed after retries"
     ]
    }
   ],
   "source": [
    "EVAL_QUESTIONS = [\n",
    "    \"What is generalization error (true risk)?\",\n",
    "    \"Define gamma-weak learnability.\",\n",
    "    \"What is a hypothesis in statistical learning?\",\n",
    "    \"What is the main idea of AdaBoost?\",\n",
    "    \"What does hard-margin SVM optimize?\",\n",
    "    \"What is uniform convergence and why is it important?\",\n",
    "    \"What does the fundamental theorem of statistical learning state?\",\n",
    "    \"How is VC dimension related to sample complexity?\",\n",
    "    \"What is the Adam optimizer?\",\n",
    "    \"Explain transformers in deep learning.\"\n",
    "]\n",
    "\n",
    "def run_eval():\n",
    "    results = []\n",
    "\n",
    "    for q in EVAL_QUESTIONS:\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"QUESTION:\", q)\n",
    "\n",
    "        llm_ans = llm_only_answer(q)\n",
    "        print(\"\\nLLM-only:\\n\", llm_ans)\n",
    "\n",
    "        for mode in [\"bm25\", \"emb\", \"hybrid\"]:\n",
    "            out = rag_answer(q, mode=mode, k=5, type_filter=\"lecture\")\n",
    "            print(f\"\\nRAG ({mode}):\\n\", out[\"answer\"])\n",
    "\n",
    "            results.append({\n",
    "                \"question\": q,\n",
    "                \"mode\": mode,\n",
    "                \"answer\": out[\"answer\"],\n",
    "                \"retrieved\": out[\"retrieved\"]\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "eval_results = run_eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
