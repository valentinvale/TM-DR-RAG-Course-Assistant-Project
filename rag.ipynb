{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0498db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DIR = \"data/aml_raw\"\n",
    "META_FILE = os.path.join(RAW_DIR, \"metadata.jsonl\")\n",
    "OUT_FILE = \"data/aml_chunks.jsonl\"\n",
    "\n",
    "MIN_CHARS = 800 # minimum number of characters per chunk\n",
    "MAX_CHARS = 1400 # maximum number of characters per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load metadata\n",
    "def load_metadata(path):\n",
    "    meta = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            meta.append(json.loads(line))\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee44d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from PDF pages\n",
    "def extract_pages(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        if text:\n",
    "            pages.append((i + 1, text))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d8d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine if a split should occur\n",
    "def should_split(text):\n",
    "    triggers = [\n",
    "        \"Definition\", \"Theorem\", \"Lemma\",\n",
    "        \"Proof\", \"Algorithm\", \"Recap\",\n",
    "        \"Today‚Äôs lecture\", \"Overview\"\n",
    "    ]\n",
    "    return any(t in text for t in triggers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5a9432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:04<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "metadata = load_metadata(META_FILE)\n",
    "\n",
    "with open(OUT_FILE, \"w\", encoding=\"utf-8\") as out:\n",
    "    for doc_meta in tqdm(metadata, desc=\"Chunking PDFs\"):\n",
    "        pdf_path = doc_meta[\"source_file\"]\n",
    "        if not os.path.exists(pdf_path):\n",
    "            continue\n",
    "\n",
    "        pages = extract_pages(pdf_path)\n",
    "\n",
    "        buffer = \"\"\n",
    "        page_start = None\n",
    "        chunk_id = 0\n",
    "\n",
    "        for page_num, text in pages:\n",
    "\n",
    "            if page_num <= 5:\n",
    "                low = text.lower()\n",
    "                if (\"administrative\" in low\n",
    "                    or \"seminar\" in low and \"room\" in low\n",
    "                    or \"uploaded\" in low\n",
    "                    or \"attendance\" in low\n",
    "                    or \"exam\" in low and \"date\" in low):\n",
    "                    \n",
    "                    continue\n",
    "\n",
    "                \n",
    "\n",
    "            if page_start is None:\n",
    "                page_start = page_num\n",
    "\n",
    "            if should_split(text) and len(buffer) >= MIN_CHARS:\n",
    "                chunk = {\n",
    "                    \"chunk_id\": f\"{doc_meta['doc_id']}_p{page_start}_c{chunk_id}\",\n",
    "                    \"doc_id\": doc_meta[\"doc_id\"],\n",
    "                    \"type\": doc_meta[\"type\"],\n",
    "                    \"index\": doc_meta[\"index\"],\n",
    "                    \"page_start\": page_start,\n",
    "                    \"page_end\": page_num - 1,\n",
    "                    \"source_file\": os.path.basename(pdf_path),\n",
    "                    \"text\": buffer.strip()\n",
    "                }\n",
    "                out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "                chunk_id += 1\n",
    "                buffer = \"\"\n",
    "                page_start = page_num\n",
    "\n",
    "            buffer += \"\\n\" + text\n",
    "\n",
    "            if len(buffer) >= MAX_CHARS:\n",
    "                chunk = {\n",
    "                    \"chunk_id\": f\"{doc_meta['doc_id']}_p{page_start}_c{chunk_id}\",\n",
    "                    \"doc_id\": doc_meta[\"doc_id\"],\n",
    "                    \"type\": doc_meta[\"type\"],\n",
    "                    \"index\": doc_meta[\"index\"],\n",
    "                    \"page_start\": page_start,\n",
    "                    \"page_end\": page_num,\n",
    "                    \"source_file\": os.path.basename(pdf_path),\n",
    "                    \"text\": buffer.strip()\n",
    "                }\n",
    "                out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "                chunk_id += 1\n",
    "                buffer = \"\"\n",
    "                page_start = None\n",
    "\n",
    "        if buffer.strip():\n",
    "            chunk = {\n",
    "                \"chunk_id\": f\"{doc_meta['doc_id']}_p{page_start}_c{chunk_id}\",\n",
    "                \"doc_id\": doc_meta[\"doc_id\"],\n",
    "                \"type\": doc_meta[\"type\"],\n",
    "                \"index\": doc_meta[\"index\"],\n",
    "                \"page_start\": page_start,\n",
    "                \"page_end\": pages[-1][0],\n",
    "                \"source_file\": os.path.basename(pdf_path),\n",
    "                \"text\": buffer.strip()\n",
    "            }\n",
    "            out.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acea0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "IN_FILE = \"data/aml_chunks.jsonl\"\n",
    "OUT_FILE = \"data/aml_chunks.cleaned.jsonl\"\n",
    "\n",
    "MIN_LEN = 80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b2a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                items.append(json.loads(line))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab0487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path, items):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in items:\n",
    "            f.write(json.dumps(it, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4639e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_into_next(chunks, min_len=80):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        cur = chunks[i]\n",
    "        cur_text = (cur.get(\"text\") or \"\").strip()\n",
    "\n",
    "        if len(cur_text) < min_len:\n",
    "            # try merge into next chunk if same doc_id\n",
    "            if i + 1 < len(chunks) and chunks[i + 1].get(\"doc_id\") == cur.get(\"doc_id\"):\n",
    "                nxt = chunks[i + 1]\n",
    "                nxt_text = (nxt.get(\"text\") or \"\").strip()\n",
    "\n",
    "                # prepend tiny text to next chunk\n",
    "                combined = (cur_text + \"\\n\" + nxt_text).strip()\n",
    "                nxt[\"text\"] = combined\n",
    "\n",
    "                # expand page range to include the tiny chunk\n",
    "                nxt[\"page_start\"] = min(cur.get(\"page_start\", nxt.get(\"page_start\")), nxt.get(\"page_start\"))\n",
    "                # page_end stays nxt's page_end (it already ends later)\n",
    "\n",
    "                # you can optionally record merged-from ids\n",
    "                merged_from = nxt.get(\"merged_from\", [])\n",
    "                merged_from.append(cur.get(\"chunk_id\"))\n",
    "                nxt[\"merged_from\"] = merged_from\n",
    "\n",
    "                # skip cur (drop it) and move to next\n",
    "                i += 1\n",
    "            else:\n",
    "                # can't merge forward ‚Üí try merge backward into previous if same doc_id\n",
    "                if out and out[-1].get(\"doc_id\") == cur.get(\"doc_id\"):\n",
    "                    prev = out[-1]\n",
    "                    prev[\"text\"] = (prev.get(\"text\",\"\").strip() + \"\\n\" + cur_text).strip()\n",
    "                    prev[\"page_end\"] = max(prev.get(\"page_end\", cur.get(\"page_end\")), cur.get(\"page_end\"))\n",
    "\n",
    "                    merged_from = prev.get(\"merged_from\", [])\n",
    "                    merged_from.append(cur.get(\"chunk_id\"))\n",
    "                    prev[\"merged_from\"] = merged_from\n",
    "                # else: drop it silently (or keep it if you prefer)\n",
    "        else:\n",
    "            out.append(cur)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6082fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned chunks written to data/aml_chunks.cleaned.jsonl, from 165 to 164 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = read_jsonl(IN_FILE)\n",
    "cleaned_chunks = merge_small_into_next(chunks, min_len=MIN_LEN)\n",
    "write_jsonl(OUT_FILE, cleaned_chunks)\n",
    "print(f\"Cleaned chunks written to {OUT_FILE}, from {len(chunks)} to {len(cleaned_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bb54a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept: 157 dropped: 7 -> data/aml_chunks.final.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "\n",
    "IN_FILE = \"data/aml_chunks.cleaned.jsonl\"\n",
    "OUT_FILE = \"data/aml_chunks.final.jsonl\"\n",
    "\n",
    "DROP_PATTERNS = [\n",
    "    r\"\\badministrative\\b\",\n",
    "    r\"\\bexam date\\b\",\n",
    "    r\"\\blecture\\b\\s*:\\s*\",\n",
    "    r\"\\bseminar\\b\\s*:\\s*\",\n",
    "    r\"\\broom\\b\\s*\\d+\",\n",
    "    r\"\\bschedule\\b\",\n",
    "]\n",
    "\n",
    "drop_re = re.compile(\"|\".join(DROP_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "kept = 0\n",
    "dropped = 0\n",
    "\n",
    "with open(IN_FILE, \"r\", encoding=\"utf-8\") as fin, open(OUT_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        text = (obj.get(\"text\") or \"\").strip()\n",
    "\n",
    "        # Only drop if it looks like logistics (early pages), to avoid false positives\n",
    "        is_early = (obj.get(\"page_end\", 9999) <= 12)\n",
    "        if is_early and drop_re.search(text):\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        kept += 1\n",
    "\n",
    "print(\"kept:\", kept, \"dropped:\", dropped, \"->\", OUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b58d1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss  # assuming this now works on your system\n",
    "\n",
    "DATA_FILE = \"data/aml_chunks.final.jsonl\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b46696ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"what\", \"is\", \"the\", \"a\", \"an\", \"in\", \"on\", \"of\", \"to\", \"and\", \"or\", \"for\", \"with\",\n",
    "    \"as\", \"this\", \"that\", \"it\", \"be\", \"are\", \"was\", \"were\", \"do\", \"does\", \"did\",\n",
    "    \"about\", \"behind\", \"explain\", \"define\", \"idea\", \"main\"\n",
    "}\n",
    "\n",
    "_word_re = re.compile(r\"[A-Za-z0-9_]+\")\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    tokens = [t.lower() for t in _word_re.findall(text)]\n",
    "    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "\n",
    "\n",
    "def load_chunks(path: str) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                chunks.append(json.loads(line))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58580331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    score: float\n",
    "    chunk: Dict[str, Any]\n",
    "    retriever: str   # üëà NEW FIELD\n",
    "\n",
    "    def cite(self) -> str:\n",
    "        src = self.chunk.get(\"source_file\", \"\")\n",
    "        ps = self.chunk.get(\"page_start\", \"\")\n",
    "        pe = self.chunk.get(\"page_end\", \"\")\n",
    "        return f\"{src} p.{ps}\" if ps == pe else f\"{src} p.{ps}-{pe}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f476ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "        self.tokens = [tokenize(c.get(\"text\",\"\")) for c in chunks]\n",
    "        self.bm25 = BM25Okapi(self.tokens)\n",
    "\n",
    "    def search(self, query, k=5, type_filter=\"lecture\"):\n",
    "        scores = self.bm25.get_scores(tokenize(query))\n",
    "        idxs = np.argsort(scores)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in idxs:\n",
    "            c = self.chunks[int(i)]\n",
    "            if type_filter and c.get(\"type\") != type_filter:\n",
    "                continue\n",
    "            results.append(\n",
    "                SearchResult(float(scores[int(i)]), c, \"bm25\")\n",
    "            )\n",
    "            if len(results) >= k:\n",
    "                break\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab797b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingRetriever:\n",
    "    def __init__(self, chunks, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.chunks = chunks\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        texts = [c.get(\"text\",\"\") for c in chunks]\n",
    "        embs = []\n",
    "        for i in tqdm(range(0, len(texts), 64), desc=\"Embedding chunks\"):\n",
    "            batch = texts[i:i+64]\n",
    "            vecs = self.model.encode(batch, normalize_embeddings=True)\n",
    "            embs.append(vecs)\n",
    "\n",
    "        self.embs = np.vstack(embs).astype(\"float32\")\n",
    "\n",
    "    def search(self, query, k=5, type_filter=\"lecture\"):\n",
    "        q = self.model.encode([query], normalize_embeddings=True)[0]\n",
    "        scores = self.embs @ q\n",
    "\n",
    "        idxs = np.argsort(scores)[::-1]\n",
    "        results = []\n",
    "        for i in idxs:\n",
    "            c = self.chunks[int(i)]\n",
    "            if type_filter and c.get(\"type\") != type_filter:\n",
    "                continue\n",
    "            results.append(\n",
    "                SearchResult(float(scores[int(i)]), c, \"emb\")\n",
    "            )\n",
    "            if len(results) >= k:\n",
    "                break\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53118fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 157 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Embedding chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "chunks = load_chunks(DATA_FILE)\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "bm25 = BM25Retriever(chunks)\n",
    "emb = EmbeddingRetriever(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "811729d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY: What is generalization error in statistical learning?\n",
      "\n",
      "=== BM25 ===\n",
      "- score=8.9361 | aml_lecture_03 | Lecture03.pdf p.1-3\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 Recap ‚Ä¢ A Formal Model ‚Äì The Statistical learning framework ‚Äì papaya tasting learning scenario, classification task: tasty ...\n",
      "- score=6.8099 | aml_lecture_01 | Lecture01.pdf p.56-61\n",
      "  ‚Ä¢ Survey of prominent methods and approaches with strong  theoretical foundations such as: ‚Äì Boosting ‚Äì SVMs ‚Äì neural networks? (loose bounds, work in progress) ‚Äì etc Course Structure ‚Äì Part 2 Usefulness of Theoretical  Machine Learning Per...\n",
      "- score=6.0135 | aml_lecture_08 | Lecture08.pdf p.1-6\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 The fundamental theorem of  statistical learning The fundamental theorem of statistical learning Theorem (The Fundamental T...\n",
      "- score=5.5245 | aml_lecture_12 | Lecture12.pdf p.20-25\n",
      "  Generalization error for AdaBoost Lemma Let B be a base class and let L(B,T ) be as defined as previously: L(B,T) = {h: Rd‚Üí{-1,1},                , w ‚àà RT, ht ‚àà B} Assume that both T and VCdim(B) are at least 3. Then, we have that: VCdim(L(...\n",
      "- score=5.4920 | aml_lecture_11 | Lecture11.pdf p.13-22\n",
      "  Toy example Toy example Toy example Toy example Toy example ‚Äì final classifier Next week: Viola-Jones face detector Main idea: ‚Äì represent local texture with efficiently computable  ‚Äúrectangular‚Äù features within window of interest ‚Äì select ...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.6118 | aml_lecture_02 | Lecture02.pdf p.8-11\n",
      "  ‚ÄúCorrect on future examples‚Äù ‚Ä¢ let f be the correct classifier, then we should find h such that h ‚âà f ‚Ä¢ one way: define the error of h w.r.t. f to be the probability that it does not predict  the correct label on a random data point x gener...\n",
      "- score=0.5332 | aml_lecture_12 | Lecture12.pdf p.18-19\n",
      "  Generalization error for AdaBoost A popular approach for constructing a weak learner is to apply the ERM  rule with respect to a base hypothesis class B (e.g., ERM over HDSd =  decision stumps in d dimensions).  Given a base hypothesis clas...\n",
      "- score=0.5020 | aml_lecture_01 | Lecture01.pdf p.56-61\n",
      "  ‚Ä¢ Survey of prominent methods and approaches with strong  theoretical foundations such as: ‚Äì Boosting ‚Äì SVMs ‚Äì neural networks? (loose bounds, work in progress) ‚Äì etc Course Structure ‚Äì Part 2 Usefulness of Theoretical  Machine Learning Per...\n",
      "- score=0.5001 | aml_lecture_04 | Lecture04.pdf p.17-19\n",
      "  Uniform Convergence ‚Ä¢ given H, the ERMH learning paradigm works as follows: ‚Äì based on a received training sample S of examples draw i.i.d from an unknown  distribution D over a domain Z = X √ó Y, ERMH evaluates the risk (error) of  each h i...\n",
      "- score=0.4959 | aml_lecture_08 | Lecture08.pdf p.1-6\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 The fundamental theorem of  statistical learning The fundamental theorem of statistical learning Theorem (The Fundamental T...\n",
      "\n",
      "QUERY: Explain gamma-weak learnability\n",
      "\n",
      "=== BM25 ===\n",
      "- score=7.7008 | aml_lecture_11 | Lecture11.pdf p.23-24\n",
      "  Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a  function mH :(0,1)‚ÜíN such that: ‚Ä¢ for every Œ¥ > 0                   (confidence) ‚Ä¢ for every labeling f ‚àà H, f ...\n",
      "- score=7.4510 | aml_lecture_12 | Lecture12.pdf p.3-5\n",
      "  Recap - Weak learnability - example Let X = R, H is the class of 3-piece classifiers (signed intervals):  H = {hŒ∏1,Œ∏2,b Œ∏1,Œ∏2 ‚ààR,Œ∏1 <Œ∏2,b ‚àà{‚àí1,+1}} hŒ∏1,Œ∏2,b(x) = +b, if x < Œ∏1 or x > Œ∏2  -b, if Œ∏1 ‚â§ x ‚â§ Œ∏2  Consider B the class of Decision ...\n",
      "- score=7.3364 | aml_lecture_11 | Lecture11.pdf p.25-26\n",
      "  Weak vs Strong learnability The fundamental theorem of learning (lecture 8) states that if a hypothesis class H has a VC dimension d, then the sample complexity of PAC learning  H satisfies Applying this with  Œµ = 1/2‚àíŒ≥ we immediately obtai...\n",
      "- score=6.9446 | aml_lecture_12 | Lecture12.pdf p.1-2\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 Recap - Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if th...\n",
      "- score=6.0494 | aml_lecture_11 | Lecture11.pdf p.27-29\n",
      "  Weak learnability - example ERMB is a Œ≥-weak learner for H, for Œ≥ = 1/12. Proof: Consider a h* =          ‚àà H that labels a training set S. Œ∏1 * Consider a distribution D over X = R. Then, we are sure that at least one of  the regions (-‚àû, ...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.7643 | aml_lecture_12 | Lecture12.pdf p.1-2\n",
      "  Advanced Machine Learning Bogdan Alexe,  bogdan.alexe@fmi.unibuc.ro University of Bucharest, 2nd semester, 2024-2025 Recap - Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if th...\n",
      "- score=0.7518 | aml_lecture_11 | Lecture11.pdf p.23-24\n",
      "  Weak learnability Definition (Œ≥-Weak-Learnability) A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a  function mH :(0,1)‚ÜíN such that: ‚Ä¢ for every Œ¥ > 0                   (confidence) ‚Ä¢ for every labeling f ‚àà H, f ...\n",
      "- score=0.6219 | aml_lecture_11 | Lecture11.pdf p.25-26\n",
      "  Weak vs Strong learnability The fundamental theorem of learning (lecture 8) states that if a hypothesis class H has a VC dimension d, then the sample complexity of PAC learning  H satisfies Applying this with  Œµ = 1/2‚àíŒ≥ we immediately obtai...\n",
      "- score=0.5537 | aml_lecture_11 | Lecture11.pdf p.27-29\n",
      "  Weak learnability - example ERMB is a Œ≥-weak learner for H, for Œ≥ = 1/12. Proof: Consider a h* =          ‚àà H that labels a training set S. Œ∏1 * Consider a distribution D over X = R. Then, we are sure that at least one of  the regions (-‚àû, ...\n",
      "- score=0.5468 | aml_lecture_12 | Lecture12.pdf p.3-5\n",
      "  Recap - Weak learnability - example Let X = R, H is the class of 3-piece classifiers (signed intervals):  H = {hŒ∏1,Œ∏2,b Œ∏1,Œ∏2 ‚ààR,Œ∏1 <Œ∏2,b ‚àà{‚àí1,+1}} hŒ∏1,Œ∏2,b(x) = +b, if x < Œ∏1 or x > Œ∏2  -b, if Œ∏1 ‚â§ x ‚â§ Œ∏2  Consider B the class of Decision ...\n",
      "\n",
      "QUERY: What is the idea behind AdaBoost?\n",
      "\n",
      "=== BM25 ===\n",
      "- score=4.1546 | aml_lecture_12 | Lecture12.pdf p.20-25\n",
      "  Generalization error for AdaBoost Lemma Let B be a base class and let L(B,T ) be as defined as previously: L(B,T) = {h: Rd‚Üí{-1,1},                , w ‚àà RT, ht ‚àà B} Assume that both T and VCdim(B) are at least 3. Then, we have that: VCdim(L(...\n",
      "- score=4.0724 | aml_lecture_12 | Lecture12.pdf p.18-19\n",
      "  Generalization error for AdaBoost A popular approach for constructing a weak learner is to apply the ERM  rule with respect to a base hypothesis class B (e.g., ERM over HDSd =  decision stumps in d dimensions).  Given a base hypothesis clas...\n",
      "- score=3.4943 | aml_lecture_12 | Lecture12.pdf p.11-17\n",
      "  Toy example D(1) Weak classifiers = vertical or horizontal half- planes = hypothesis from HDS 2 Toy example ‚Äì Round 1 D(1) D(2) Œµ1 = 0.30 w1 = 0.42 Weak classifiers = vertical or horizontal half- planes = hypothesis from HDS 2 Toy example ‚Äì...\n",
      "- score=3.2513 | aml_lecture_11 | Lecture11.pdf p.34-35\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "- score=3.2240 | aml_lecture_12 | Lecture12.pdf p.9-10\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.4530 | aml_lecture_01 | Lecture01.pdf p.62-64\n",
      "  AdaBoost Algorithm Start with  uniform weights  on training  examples Evaluate weighted  error for each  feature, pick best. Re-weight the examples: Incorrectly classified -> more weight Correctly classified -> less weight Final classifier ...\n",
      "- score=0.3607 | aml_lecture_12 | Lecture12.pdf p.18-19\n",
      "  Generalization error for AdaBoost A popular approach for constructing a weak learner is to apply the ERM  rule with respect to a base hypothesis class B (e.g., ERM over HDSd =  decision stumps in d dimensions).  Given a base hypothesis clas...\n",
      "- score=0.3340 | aml_lecture_11 | Lecture11.pdf p.36-41\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "- score=0.3158 | aml_lecture_12 | Lecture12.pdf p.40-45\n",
      "  AdaBoost Algorithm Start with  uniform weights  on training  examples Evaluate weighted  error for each  feature, pick best. Re-weight the examples: Incorrectly classified -> more weight Correctly classified -> less weight Final classifier ...\n",
      "- score=0.3038 | aml_lecture_11 | Lecture11.pdf p.34-35\n",
      "  AdaBoost ‚Ä¢ construct distribution D(t) on {1,..., m}: ‚Ä¢ D(1)(i) = 1/m ‚Ä¢ given D(t) and ht: where Zt+1 normalization factor (D(t+1) is a distribution):      wt is a weight:                                  as the error Œµt < 0.5    Œµt is the ...\n",
      "\n",
      "QUERY: What does hard-margin SVM optimize?\n",
      "\n",
      "=== BM25 ===\n",
      "- score=20.2331 | aml_lecture_13 | Lecture13.pdf p.20-25\n",
      "  Hard SVM Hard-SVM is the learning rule in which we return an ERM hyperplane  that separates the training set  S = {(x1, y1), (x1, y1), ‚Ä¶, (xm, ym)} with the  largest possible margin.    maximize subject to   Œ≥ = 1 w Œ≥ y(i) x(i), w w + b w ‚éõ...\n",
      "- score=17.9671 | aml_lecture_13 | Lecture13.pdf p.16-19\n",
      "  https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf Sample complexity for separability case ‚Ä¢ if the number of examples is significantly smaller than d/Œµ then no  algorithm can learn an accurate halfspace.  ‚Äì this is problema...\n",
      "- score=17.9224 | aml_lecture_13 | Lecture13.pdf p.26-28\n",
      "  Sample complexity for separability case If S = {(x1,y1), (x2,y2),‚Ä¶, (xm,ym)} separable with margin Œ≥ then we have that  S‚Äô = {(Œ±x1,y1), (Œ±x2,y2),‚Ä¶, (Œ±xm,ym)} is separable with margin Œ±Œ≥ for any Œ± > 0.   It can be shown that the sample compl...\n",
      "- score=16.8881 | aml_lecture_13 | Lecture13.pdf p.29-33\n",
      "  Soft SVM ‚Ä¢ this can be modelled by introducing nonnegative slack variables ùùÉ1,  ùùÉ2,‚Ä¶, ùùÉm, and replacing each constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 by  the constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 ‚àíùúâ! That is,  ùùÉi measures by  how much the constraint ùë¶! < ùë§, ùë•! ...\n",
      "- score=4.8534 | aml_lecture_11 | Lecture11.pdf p.6-8\n",
      "  Improper learning Hardness of learning Some classes are hard to PAC learn if we place certain restrictions on the hypothesis  class used by the learning algorithm ‚Ä¢ the problem of properly learning 3-term DNF formulae is computationally har...\n",
      "\n",
      "=== Embeddings ===\n",
      "- score=0.6913 | aml_lecture_13 | Lecture13.pdf p.20-25\n",
      "  Hard SVM Hard-SVM is the learning rule in which we return an ERM hyperplane  that separates the training set  S = {(x1, y1), (x1, y1), ‚Ä¶, (xm, ym)} with the  largest possible margin.    maximize subject to   Œ≥ = 1 w Œ≥ y(i) x(i), w w + b w ‚éõ...\n",
      "- score=0.6591 | aml_lecture_13 | Lecture13.pdf p.26-28\n",
      "  Sample complexity for separability case If S = {(x1,y1), (x2,y2),‚Ä¶, (xm,ym)} separable with margin Œ≥ then we have that  S‚Äô = {(Œ±x1,y1), (Œ±x2,y2),‚Ä¶, (Œ±xm,ym)} is separable with margin Œ±Œ≥ for any Œ± > 0.   It can be shown that the sample compl...\n",
      "- score=0.6573 | aml_lecture_13 | Lecture13.pdf p.29-33\n",
      "  Soft SVM ‚Ä¢ this can be modelled by introducing nonnegative slack variables ùùÉ1,  ùùÉ2,‚Ä¶, ùùÉm, and replacing each constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 by  the constraint ùë¶! < ùë§, ùë•! > +ùëè‚â•1 ‚àíùúâ! That is,  ùùÉi measures by  how much the constraint ùë¶! < ùë§, ùë•! ...\n",
      "- score=0.5352 | aml_lecture_13 | Lecture13.pdf p.16-19\n",
      "  https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf Sample complexity for separability case ‚Ä¢ if the number of examples is significantly smaller than d/Œµ then no  algorithm can learn an accurate halfspace.  ‚Äì this is problema...\n",
      "- score=0.4721 | aml_lecture_13 | Lecture13.pdf p.8-15\n",
      "  Support Vector Machines  (SVMs) SVMs Slide credit Sparktech SVMs Slide credit Sparktech SVMs Slide credit Sparktech SVMs Slide credit Sparktech Recap: The fundamental theorem of statistical learning  ‚Äì quantitative version Theorem Let H be ...\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(label, results, max_chars=240):\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    for r in results:\n",
    "        text = r.chunk[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        print(f\"- score={r.score:.4f} | {r.chunk['doc_id']} | {r.cite()}\")\n",
    "        print(f\"  {text[:max_chars]}{'...' if len(text) > max_chars else ''}\")\n",
    "\n",
    "queries = [\n",
    "    \"What is generalization error in statistical learning?\",\n",
    "    \"Explain gamma-weak learnability\",\n",
    "    \"What is the idea behind AdaBoost?\",\n",
    "    \"What does hard-margin SVM optimize?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQUERY: {q}\")\n",
    "    pretty_print(\"BM25\", bm25.search(q))\n",
    "    pretty_print(\"Embeddings\", emb.search(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf64cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fuse(\n",
    "    bm25_res: List[SearchResult],\n",
    "    emb_res: List[SearchResult],\n",
    "    k: int = 5,\n",
    "    rrf_k: int = 60\n",
    ") -> List[SearchResult]:\n",
    "    # key by chunk_id so identical chunks merge\n",
    "    def key(r: SearchResult) -> str:\n",
    "        return r.chunk[\"chunk_id\"]\n",
    "\n",
    "    scores = {}\n",
    "    best_obj = {}\n",
    "\n",
    "    for rank, r in enumerate(bm25_res, start=1):\n",
    "        kid = key(r)\n",
    "        scores[kid] = scores.get(kid, 0.0) + 1.0 / (rrf_k + rank)\n",
    "        best_obj[kid] = r\n",
    "\n",
    "    for rank, r in enumerate(emb_res, start=1):\n",
    "        kid = key(r)\n",
    "        scores[kid] = scores.get(kid, 0.0) + 1.0 / (rrf_k + rank)\n",
    "        # keep one representative\n",
    "        best_obj.setdefault(kid, r)\n",
    "\n",
    "    fused = []\n",
    "    for kid, sc in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        r = best_obj[kid]\n",
    "        fused.append(SearchResult(score=float(sc), chunk=r.chunk, retriever=\"hybrid\"))\n",
    "        if len(fused) >= k:\n",
    "            break\n",
    "    return fused\n",
    "\n",
    "def retrieve(query: str, mode: str = \"hybrid\", k: int = 5, type_filter: str = \"lecture\") -> List[SearchResult]:\n",
    "    if mode == \"bm25\":\n",
    "        return bm25.search(query, k=k, type_filter=type_filter)\n",
    "    if mode == \"emb\":\n",
    "        return emb.search(query, k=k, type_filter=type_filter)\n",
    "    if mode == \"hybrid\":\n",
    "        b = bm25.search(query, k=max(k, 8), type_filter=type_filter)\n",
    "        e = emb.search(query, k=max(k, 8), type_filter=type_filter)\n",
    "        return rrf_fuse(b, e, k=k)\n",
    "    raise ValueError(\"mode must be one of: bm25, emb, hybrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "252a3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, math\n",
    "import numpy as np\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DATA_FILE = \"data/aml_chunks.final.jsonl\"\n",
    "\n",
    "GROQ_BASE_URL=\"https://api.groq.com/openai/v1/\"\n",
    "GROQ_API_KEY = json.load(open(\"secrets.json\")).get(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL = \"llama-3.3-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddde2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(results: List[SearchResult], max_chars: int = 6000) -> str:\n",
    "    parts = []\n",
    "    total = 0\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        txt = (r.chunk.get(\"text\",\"\") or \"\").strip()\n",
    "        cite = r.cite()\n",
    "        block = f\"[{i}] ({cite})\\n{txt}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a course assistant for Advanced Machine Learning.\n",
    "Rules:\n",
    "- Answer ONLY using the provided context snippets.\n",
    "- If the context does not contain the answer, say: \"Not in the provided course materials.\"\n",
    "- Cite sources inline using the snippet numbers like [1], [2] and include the file+page already shown in those snippets.\n",
    "- Do not invent equations that are not explicitly present in the context. Prefer conceptual explanations.\n",
    "- Be concise and accurate.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c14bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def groq_chat(\n",
    "    messages,\n",
    "    model=GROQ_MODEL,\n",
    "    temperature=0.2,\n",
    "    max_tokens=400,\n",
    "    max_retries=5\n",
    ") -> str:\n",
    "    url = f\"{GROQ_BASE_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Rate limited. Sleeping {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    raise RuntimeError(\"Groq API failed after retries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a366822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(\n",
    "    query: str,\n",
    "    mode: str = \"hybrid\",      # \"bm25\" | \"emb\" | \"hybrid\"\n",
    "    k: int = 5,\n",
    "    type_filter: str = \"lecture\",\n",
    "    model: str = GROQ_MODEL\n",
    ") -> dict:\n",
    "    results = retrieve(query, mode=mode, k=k, type_filter=type_filter)\n",
    "    context = build_context(results)\n",
    "\n",
    "    user_prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Context snippets:\n",
    "{context}\n",
    "\n",
    "Write the answer now following the rules.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    answer = groq_chat(messages, model=model, temperature=0.2, max_tokens=450)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"mode\": mode,\n",
    "        \"k\": k,\n",
    "        \"type_filter\": type_filter,\n",
    "        \"retrieved\": [{\n",
    "            \"rank\": i+1,\n",
    "            \"retriever\": r.retriever,\n",
    "            \"score\": r.score,\n",
    "            \"chunk_id\": r.chunk[\"chunk_id\"],\n",
    "            \"doc_id\": r.chunk[\"doc_id\"],\n",
    "            \"cite\": r.cite(),\n",
    "        } for i, r in enumerate(results)],\n",
    "        \"answer\": answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "959509a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODE: bm25\n",
      "The generalization error (true risk) in statistical learning is defined as LD,f (h), which is the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is also referred to as the true error of h or the real risk of h. This error is unknown to the learner as they do not know the distribution D and the target function f [1] (Lecture02.pdf p.8-11).\n",
      "\n",
      "================================================================================\n",
      "MODE: emb\n",
      "The generalization error, also known as the true risk, is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and represents the true error of h. The goal is to find a hypothesis h such that LD,f (h) is small, which means that h generalizes well to new, unseen data [1] (Lecture02.pdf p.8-11).\n",
      "\n",
      "================================================================================\n",
      "MODE: hybrid\n",
      "The generalization error (true risk) in statistical learning is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and represents the true error of h, or the real risk of h. This error is unknown to the learner as they do not know the distribution D and the target function f [1] (Lecture02.pdf p.8-11).\n"
     ]
    }
   ],
   "source": [
    "q = \"What is generalization error (true risk) in statistical learning?\"\n",
    "for mode in [\"bm25\", \"emb\", \"hybrid\"]:\n",
    "    out = rag_answer(q, mode=mode, k=5, type_filter=\"lecture\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODE:\", mode)\n",
    "    print(out[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95c8e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_only_answer(query: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the question as best as you can.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    return groq_chat(messages, temperature=0.7, max_tokens=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e922f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "QUESTION: What is generalization error (true risk)?\n",
      "\n",
      "LLM-only:\n",
      " Generalization error, also known as true risk, is a concept in machine learning and statistical modeling that refers to the difference between the expected performance of a model on a training dataset and its performance on unseen, new data.\n",
      "\n",
      "In other words, generalization error measures how well a model is able to generalize its learning from the training data to make accurate predictions on new, unseen data. It is a measure of the model's ability to avoid overfitting, which occurs when a model is too complex and fits the training data too closely, but fails to perform well on new data.\n",
      "\n",
      "The generalization error is typically denoted as:\n",
      "\n",
      "R(h) = E[(h(x) - y)^2]\n",
      "\n",
      "where:\n",
      "- R(h) is the true risk or generalization error\n",
      "- h(x) is the prediction made by the model\n",
      "- y is the true label or value\n",
      "- E is the expected value over the distribution of the data\n",
      "\n",
      "The goal of machine learning is to minimize the generalization error, which means finding a model that performs well on both the training data and new, unseen data.\n",
      "\n",
      "There are several types of generalization error, including:\n",
      "\n",
      "1. **True risk**: The expected error of a model on the entire population of data.\n",
      "2. **Empirical risk**: The average error of a model on the training data.\n",
      "3. **Generalization gap**: The difference between the true risk and the empirical risk.\n",
      "\n",
      "Understanding and minimizing generalization error is crucial in machine learning, as\n",
      "\n",
      "RAG (bm25):\n",
      " The generalization error (true risk) is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying (unknown) probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and is called the generalization error, the true error of h, or the real risk of h. The goal is to find a hypothesis h such that LD,f (h) is small.\n",
      "\n",
      "RAG (emb):\n",
      " The generalization error, also known as the true risk, is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and represents the true error of h. In other words, it measures how well the hypothesis h approximates the correct classifier f. The generalization error is unknown to the learner as it depends on the unknown distribution D and the target function f [1] (Lecture02.pdf p.8-11).\n",
      "\n",
      "RAG (hybrid):\n",
      " The generalization error, also known as the true risk, is defined as the probability that a hypothesis h does not predict the correct label on a random data point x generated by the underlying probability distribution D over X [1] (Lecture02.pdf p.8-11). It is denoted as LD,f (h) and represents the true error of h. The goal is to find a hypothesis h such that LD,f (h) is small, which means that h generalizes well to new, unseen data [1] (Lecture02.pdf p.8-11).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: Define gamma-weak learnability.\n",
      "\n",
      "LLM-only:\n",
      " A question from the realm of computational learning theory.\n",
      "\n",
      "Gamma-weak learnability is a concept in machine learning that was introduced by Michael Kearns and Leslie Valiant in 1988, as a relaxation of the standard notion of weak learnability.\n",
      "\n",
      "**Weak learnability**:\n",
      "A learning algorithm is said to be weakly learnable if, given a set of examples from an unknown distribution, it can output a hypothesis that is slightly better than random guessing. More formally, a learning algorithm is weakly learnable if it can achieve an error rate of at most 1/2 - Œ≥, where Œ≥ is a small positive constant.\n",
      "\n",
      "**Gamma-weak learnability**:\n",
      "A learning algorithm is said to be gamma-weakly learnable if, given a set of examples from an unknown distribution, it can output a hypothesis that achieves an error rate of at most 1/2 - Œ≥, where Œ≥ is a function of the sample size (n) and the confidence parameter (Œ¥). In other words, the algorithm can learn to achieve an error rate that is slightly better than random guessing, but the amount of improvement (Œ≥) depends on the sample size and the desired confidence level.\n",
      "\n",
      "Formally, a concept class C is gamma-weakly learnable if there exists a learning algorithm A and a function Œ≥(n, Œ¥) > 0 such that, for any distribution D over the instance space, any target concept c ‚àà C, and any Œ¥ > 0, with probability at\n",
      "\n",
      "RAG (bm25):\n",
      " A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a function mH :(0,1)‚ÜíN such that for every Œ¥ > 0, for every labeling f ‚àà H, and for every distribution D over X, when we run A on a training set of m ‚â• mH (Œ¥) examples sampled i.i.d. from D and labeled by f, A returns a hypothesis h such that, with probability at least 1‚àíŒ¥, LD,f (h) ‚â§ 1/2 - Œ≥ [1] (Lecture11.pdf p.23-24). \n",
      "In other words, a hypothesis class H is Œ≥-weak-learnable if there exists a Œ≥-weak-learner for that class, meaning the algorithm can output a hypothesis whose error rate is at most 1/2‚àíŒ≥, which is slightly better than what a random labeling would give us [2] (Lecture12.pdf p.3-5).\n",
      "\n",
      "RAG (emb):\n",
      " A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a function mH :(0,1)‚ÜíN such that: for every Œ¥ > 0, for every labeling f ‚àà H, f : X ‚Üí {-1,+1}, and for every distribution D over X, when we run the learning algorithm A on a training set, consisting of m ‚â• mH (Œ¥) examples sampled i.i.d. from D and labeled by f, the algorithm A returns a hypothesis h such that, with probability at least 1‚àíŒ¥ (over the choice of examples), LD,f (h) ‚â§ 1/2 - Œ≥ [1] (Lecture12.pdf p.1-2). A hypothesis class H is Œ≥-weak-learnable if there exists a Œ≥-weak-learner for that class [2] (Lecture11.pdf p.23-24).\n",
      "\n",
      "RAG (hybrid):\n",
      " A learning algorithm, A, is a Œ≥-weak-learner for a class H if there exists a function mH :(0,1)‚ÜíN such that: for every Œ¥ > 0, for every labeling f ‚àà H, f : X ‚Üí {-1,+1}, and for every distribution D over X, when we run the learning algorithm A on a training set, consisting of m ‚â• mH (Œ¥) examples sampled i.i.d. from D and labeled by f, the algorithm A returns a hypothesis h such that, with probability at least 1‚àíŒ¥, LD,f (h) ‚â§ 1/2 - Œ≥ [1] (Lecture11.pdf p.23-24), [2] (Lecture12.pdf p.1-2). A hypothesis class H is Œ≥-weak-learnable if there exists a Œ≥-weak-learner for that class. In weak learnability, we only need to output a hypothesis whose error rate is at most 1/2‚àíŒ≥, namely, whose error rate is slightly better than what a random labeling would give us [1] (Lecture11.pdf p.23-24), [3] (Lecture12.pdf p.3-5).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: What is a hypothesis in statistical learning?\n",
      "\n",
      "LLM-only:\n",
      " In statistical learning, a hypothesis is a proposed explanation or model that attempts to describe the relationship between a set of input variables (also known as features or predictors) and a target variable (also known as the response variable or outcome). It is a statement or equation that predicts the value of the target variable based on the values of the input variables.\n",
      "\n",
      "A hypothesis can be thought of as a \"guess\" or a \"proposal\" about the underlying pattern or relationship in the data. The goal of statistical learning is to evaluate and refine this hypothesis using data, in order to develop a more accurate and reliable model.\n",
      "\n",
      "A hypothesis typically consists of two components:\n",
      "\n",
      "1. **Model**: A mathematical function or equation that describes the relationship between the input variables and the target variable. For example, a linear regression model might be represented by the equation y = Œ≤0 + Œ≤1x + Œµ, where y is the target variable, x is the input variable, Œ≤0 and Œ≤1 are coefficients, and Œµ is the error term.\n",
      "2. **Parameters**: The coefficients or weights that are used to fit the model to the data. In the example above, Œ≤0 and Œ≤1 are the parameters of the model.\n",
      "\n",
      "The hypothesis is typically evaluated using a dataset, and the goal is to find the best-fitting model that minimizes the difference between the observed values and the predicted values. This is often done using techniques such as maximum likelihood estimation, least squares estimation, or cross-validation.\n",
      "\n",
      "There are different types of\n",
      "\n",
      "RAG (bm25):\n",
      " A hypothesis in statistical learning is a function from a domain X to {0,1} [1] (Lecture08.pdf p.1-6), [2] (Lecture09.pdf p.1-3), and [3] (Lecture07.pdf p.29-30). It is part of a hypothesis class H, which is a set of such functions. \n",
      "\n",
      "In the context of the provided course materials, a hypothesis is often denoted as h and is typically an output of a learning rule, where the learning rule is consistent if it outputs a hypothesis h that perfectly fits the data, i.e., h(xi) = yi for all (xi, yi) in the sample S [4] (Lecture10.pdf p.8).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (emb):\n",
      " A hypothesis in statistical learning is a classifier or predictor, denoted as h, that is learned from the training data S, with the goal of approximating the correct classifier, f [1] (Lecture02.pdf p.8-11). The hypothesis h is returned by a learning algorithm A based on the training sequence S, and its quality is measured by its generalization error, LD,f (h), which is the probability that it does not predict the correct label on a random data point x generated by the underlying probability distribution D [1] (Lecture02.pdf p.8-11). In other words, the hypothesis h is a solution that works well on the training data, and the goal is to find a hypothesis that minimizes the empirical risk, LS(h), which is the error on the training data [1] (Lecture02.pdf p.8-11).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (hybrid):\n",
      " A hypothesis in statistical learning is a function from a domain X to {0,1} [1] (Lecture08.pdf p.1-6), which is used to predict the correct label on a random data point x generated by the underlying probability distribution D over X [2] (Lecture02.pdf p.8-11). It is denoted as h and is returned by a learning algorithm A based on the training sequence S [2] (Lecture02.pdf p.8-11). The goal is to find a hypothesis h that is close to the correct classifier f, such that the generalization error LD,f (h) is small [2] (Lecture02.pdf p.8-11).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: What is the main idea of AdaBoost?\n",
      "\n",
      "LLM-only:\n",
      " The main idea of AdaBoost (Adaptive Boosting) is a machine learning algorithm that combines multiple weak models to create a strong predictive model. The core concept is to iteratively train a series of weak models, with each subsequent model attempting to correct the errors of the previous model.\n",
      "\n",
      "Here's a simplified overview of the AdaBoost process:\n",
      "\n",
      "1. **Initialize weights**: Assign equal weights to all training samples.\n",
      "2. **Train a weak model**: Train a weak model (e.g., a decision tree) on the weighted training data.\n",
      "3. **Calculate errors**: Calculate the error rate of the weak model on the training data.\n",
      "4. **Update weights**: Update the weights of the training samples based on the errors. Samples that are misclassified by the weak model have their weights increased, while samples that are correctly classified have their weights decreased.\n",
      "5. **Repeat steps 2-4**: Train a new weak model on the updated weighted training data, and repeat the process.\n",
      "6. **Combine models**: Combine the predictions of all the weak models, with each model's prediction weighted by its performance (i.e., its error rate).\n",
      "\n",
      "The key intuition behind AdaBoost is that each weak model focuses on the samples that are hardest to classify, and the subsequent models attempt to correct the errors of the previous models. By combining the predictions of multiple weak models, AdaBoost creates a strong predictive model that can achieve high accuracy and robustness.\n",
      "\n",
      "AdaBoost has several benefits, including:\n",
      "\n",
      "* **Handling\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (bm25):\n",
      " The main idea of AdaBoost is to combine multiple weak learners to create a strong learner, with the goal of reducing the generalization error [1] (Lecture12.pdf p.20-25). AdaBoost works by iteratively adding weak learners, with each subsequent learner attempting to correct the errors of the previous ones [3] (Lecture12.pdf p.11-17). The number of rounds of boosting, T, acts as a complexity control, allowing for a trade-off between the approximation error and the estimation error [2] (Lecture12.pdf p.18-19). As T grows, the approximation error decreases, but the estimation error increases, and the VC dimension of the hypothesis class also grows [1] (Lecture12.pdf p.20-25).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (emb):\n",
      " The main idea of AdaBoost is to combine multiple weak classifiers to create a strong classifier [1] (Lecture01.pdf p.62-64). It starts with uniform weights on training examples, evaluates the weighted error for each feature, and picks the best one. Then, it re-weights the examples, increasing the weight for incorrectly classified examples and decreasing the weight for correctly classified ones [1] (Lecture01.pdf p.62-64), [3] (Lecture11.pdf p.36-41). The final classifier is a combination of the weak ones, weighted according to the error they had [1] (Lecture01.pdf p.62-64), [3] (Lecture11.pdf p.36-41).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (hybrid):\n",
      " The main idea of AdaBoost is to combine multiple weak learners to create a strong learner [2] (Lecture01.pdf p.62-64). It works by iteratively training weak learners on a weighted version of the training data, where the weights are adjusted based on the error of the previous weak learner [3] (Lecture11.pdf p.34-35). The final classifier is a weighted combination of the weak learners, where the weights are determined by the error of each weak learner [1] (Lecture12.pdf p.18-19). This process enables AdaBoost to control the bias-complexity tradeoff and achieve a low generalization error [1] (Lecture12.pdf p.18-19).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: What does hard-margin SVM optimize?\n",
      "\n",
      "LLM-only:\n",
      " Hard-margin Support Vector Machine (SVM) optimizes the margin between classes, which is the distance between the decision boundary and the nearest data points of each class. The goal is to maximize this margin, thereby maximizing the separation between classes.\n",
      "\n",
      "Mathematically, hard-margin SVM solves the following optimization problem:\n",
      "\n",
      "Maximize: `2 / ||w||`\n",
      "\n",
      "subject to: `y_i (w^T x_i + b) ‚â• 1` for all `i`\n",
      "\n",
      "where:\n",
      "\n",
      "* `w` is the weight vector\n",
      "* `x_i` is the `i-th` data point\n",
      "* `y_i` is the label of the `i-th` data point (`+1` or `-1`)\n",
      "* `b` is the bias term\n",
      "* `||.||` denotes the Euclidean norm\n",
      "\n",
      "The constraint `y_i (w^T x_i + b) ‚â• 1` ensures that all data points are correctly classified and lie on the correct side of the decision boundary. The optimization problem maximizes the margin `2 / ||w||`, which is equivalent to minimizing `||w||`, since the margin is inversely proportional to the norm of the weight vector.\n",
      "\n",
      "By solving this optimization problem, hard-margin SVM finds the optimal hyperplane that maximally separates the classes, resulting in the largest possible margin.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "Rate limited. Sleeping 8s...\n",
      "\n",
      "RAG (bm25):\n",
      " Hard-margin SVM optimizes the margin, which is the distance between the hyperplane and the nearest data points [1] (Lecture13.pdf p.20-25). It can be formulated as maximizing the margin Œ≥, subject to the constraints y(i) < x(i), w > + b ‚â• Œ≥ [1]. Alternatively, it can also be viewed as minimizing the norm of w (||w||) subject to the constraints y(i) < x(i), w > + b ‚â• 1 [2] (Lecture13.pdf p.16-19).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "\n",
      "RAG (emb):\n",
      " Hard-margin SVM optimizes the margin, which is the distance between the hyperplane and the nearest data points [1] (Lecture13.pdf p.20-25). It returns an ERM hyperplane that separates the training set with the largest possible margin. This can be formulated as maximizing the margin Œ≥, subject to the constraints y(i) * (w * x(i) + b) ‚â• Œ≥, where w is the weight vector and b is the bias term [1] (Lecture13.pdf p.20-25).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "Rate limited. Sleeping 8s...\n",
      "\n",
      "RAG (hybrid):\n",
      " Hard-margin SVM optimizes the margin, which is the distance between the hyperplane and the nearest data points [1] (Lecture13.pdf p.20-25). It returns an ERM hyperplane that separates the training set with the largest possible margin. This can be formulated as maximizing the margin Œ≥, subject to the constraints y(i) * (w * x(i) + b) ‚â• Œ≥ [1] (Lecture13.pdf p.20-25).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: What is uniform convergence and why is it important?\n",
      "\n",
      "LLM-only:\n",
      " Uniform convergence is a fundamental concept in mathematical analysis, particularly in the study of sequences and series of functions. It's a way to describe how a sequence of functions converges to a limit function.\n",
      "\n",
      "**Definition:**\n",
      "\n",
      "Given a sequence of functions {f_n} and a function f, we say that {f_n} converges uniformly to f on a set E if for every Œµ > 0, there exists an integer N such that for all n ‚â• N and all x ‚àà E, we have:\n",
      "\n",
      "|f_n(x) - f(x)| < Œµ\n",
      "\n",
      "In other words, uniform convergence means that the sequence {f_n} converges to f at the same rate for all x in the set E. The difference between f_n(x) and f(x) becomes arbitrarily small as n increases, and this happens simultaneously for all x in E.\n",
      "\n",
      "**Importance:**\n",
      "\n",
      "Uniform convergence is important for several reasons:\n",
      "\n",
      "1. **Preservation of properties:** Uniform convergence preserves many properties of the limit function, such as continuity, differentiability, and integrability. This means that if a sequence of functions converges uniformly to a function with certain properties, the limit function will also have those properties.\n",
      "2. **Exchange of limits:** Uniform convergence allows us to exchange the order of limits in certain situations. For example, if {f_n} converges uniformly to f, then we can exchange the limit and the integral: ‚à´f_n(x) dx ‚Üí ‚à´f(x) dx\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (bm25):\n",
      " Uniform convergence is a property of a hypothesis class H, defined as having a function mH UC :(0,1)2 ‚ÜíŒù, such that for all (Œµ, Œ¥) ‚àà (0,1)2 and for any probability distribution D over Z, if S is a sample of m ‚â• mH UC(Œµ,Œ¥) examples drawn i.i.d. according to D, then, with probability of at least 1 ‚Äì Œ¥, S is Œµ-representative [1] (Lecture08.pdf p.7), [2] (Lecture04.pdf p.20-22). \n",
      "\n",
      "It is important because it serves as a tool to prove that we can PAC learn a hypothesis class H [2] (Lecture04.pdf p.20-22). If a hypothesis class H has the uniform convergence property, then H is agnostically PAC learnable with the sample complexity: mH UC(Œµ,Œ¥) ‚â§ mH UC(Œµ / 2,Œ¥) [2] (Lecture04.pdf p.20-22). Moreover, the ERMH paradigm is a successful agnostic PAC learner for H. \n",
      "\n",
      "Uniform convergence is also equivalent to having a finite VC-dimension, which characterizes PAC learnability [5] (Lecture07.pdf p.29-30). A finite VC-dimension guarantees learnability, and the VC-dimension is a measure of the complexity of a hypothesis class. \n",
      "\n",
      "In summary, uniform convergence is a fundamental concept in statistical learning, and it is essential for proving the learnability of a hypothesis class. It is closely related to the VC-dimension, and both concepts are crucial for understanding the theoretical foundations of machine learning.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (emb):\n",
      " Uniform convergence is a property of a hypothesis class H, where there exists a function mH UC : (0,1)2 ‚Üí Œù, such that for all (Œµ, Œ¥) ‚àà (0,1)2 and for any probability distribution D over a domain Z, if a sample S of size m ‚â• mH UC(Œµ, Œ¥) is drawn i.i.d. according to D, then with probability of at least 1 ‚Äì Œ¥, S is Œµ-representative [1] (Lecture08.pdf p.7). \n",
      "\n",
      "This means that the empirical risk of all hypotheses in H will be close to their true risk, uniformly over all possible probability distributions D [2] (Lecture04.pdf p.17-19). Uniform convergence is important because it serves as a tool to prove that a hypothesis class H is agnostically PAC learnable [3] (Lecture04.pdf p.20-22). \n",
      "\n",
      "In other words, if a hypothesis class has the uniform convergence property, then the Empirical Risk Minimization (ERM) paradigm is a successful agnostic PAC learner for H, and the sample complexity can be bounded [3] (Lecture04.pdf p.20-22). This is crucial in machine learning as it guarantees that the learned hypothesis will generalize well to new, unseen data.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (hybrid):\n",
      " Uniform convergence refers to a property of a hypothesis class H, where there exists a function mH UC such that for all (Œµ, Œ¥) ‚àà (0,1)2 and for any probability distribution D over the domain Z, if S is a sample of m ‚â• mH UC(Œµ, Œ¥) examples drawn i.i.d. according to D, then, with probability of at least 1 ‚Äì Œ¥, S is Œµ-representative [1] (Lecture08.pdf p.7), [2] (Lecture04.pdf p.20-22).\n",
      "\n",
      "Uniform convergence is important because it serves as a tool to prove that we can PAC learn a hypothesis class H [2] (Lecture04.pdf p.20-22). If a hypothesis class H has the uniform convergence property, then H is agnostically PAC learnable [3] (Lecture07.pdf p.29-30). Moreover, the ERMH paradigm is a successful agnostic PAC learner for H [2] (Lecture04.pdf p.20-22). A finite VC-dimension guarantees learnability and is equivalent to the uniform convergence property [3] (Lecture07.pdf p.29-30). \n",
      "\n",
      "In other words, uniform convergence is crucial for learning because it ensures that with a sufficiently large sample size, we can learn a hypothesis that is close to the optimal one, with high probability [2] (Lecture04.pdf p.20-22). This property is essential for PAC learnability, which is a fundamental concept in machine learning [3] (Lecture07.pdf p.29-30).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: What does the fundamental theorem of statistical learning state?\n",
      "\n",
      "LLM-only:\n",
      " The Fundamental Theorem of Statistical Learning, also known as the Bias-Variance Tradeoff or the Fundamental Theorem of Machine Learning, states that the error of a learning algorithm can be decomposed into three main components:\n",
      "\n",
      "1. **Bias**: This refers to the error introduced by the simplifying assumptions made by the model. A high bias means that the model is oversimplified and fails to capture the underlying patterns in the data.\n",
      "2. **Variance**: This refers to the error introduced by the noise or randomness in the data. A high variance means that the model is overfitting to the training data and is not generalizing well to new, unseen data.\n",
      "3. **Irreducible error** (or **noise**): This refers to the error that is inherent in the data itself and cannot be reduced by any model. This includes errors due to measurement noise, sampling variability, and other sources of randomness.\n",
      "\n",
      "The theorem states that the expected error of a learning algorithm can be written as:\n",
      "\n",
      "Expected Error = Bias^2 + Variance + Irreducible Error\n",
      "\n",
      "The goal of statistical learning is to find a balance between bias and variance, as reducing one often increases the other. This tradeoff is known as the bias-variance tradeoff.\n",
      "\n",
      "In essence, the Fundamental Theorem of Statistical Learning highlights the importance of finding a model that is complex enough to capture the underlying patterns in the data (low bias), but not so complex that it overfits to the noise in the data (low\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (bm25):\n",
      " The fundamental theorem of statistical learning states that the following statements are equivalent: \n",
      "1. H has the uniform convergence property, \n",
      "2. Any ERM rule is a successful agnostic PAC learner for H, \n",
      "3. H is agnostic PAC learnable, \n",
      "4. H is PAC learnable, \n",
      "5. Any ERM rule is a successful PAC learner for H, \n",
      "6. H has a finite VC-dimension [1] (Lecture08.pdf p.1-6), [2] (Lecture07.pdf p.29-30), [4] (Lecture09.pdf p.1-3). \n",
      "A finite VC-dimension guarantees learnability, and hence, the VC-dimension characterizes PAC learnability.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (emb):\n",
      " The fundamental theorem of statistical learning states that the following are equivalent: \n",
      "1. H has the uniform convergence property, \n",
      "2. Any ERM rule is a successful agnostic PAC learner for H, \n",
      "3. H is agnostic PAC learnable, \n",
      "4. H is PAC learnable, \n",
      "5. Any ERM rule is a successful PAC learner for H, \n",
      "6. H has a finite VC-dimension [1] (Lecture09.pdf p.1-3), [3] (Lecture08.pdf p.1-6). \n",
      "A finite VC-dimension guarantees learnability, and hence, the VC-dimension characterizes PAC learnability [1] (Lecture09.pdf p.1-3).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (hybrid):\n",
      " The fundamental theorem of statistical learning states that the following statements are equivalent: \n",
      "1. H has the uniform convergence property, \n",
      "2. Any ERM rule is a successful agnostic PAC learner for H, \n",
      "3. H is agnostic PAC learnable, \n",
      "4. H is PAC learnable, \n",
      "5. Any ERM rule is a successful PAC learner for H, \n",
      "6. H has a finite VC-dimension [1] (Lecture08.pdf p.1-6), [2] (Lecture09.pdf p.1-3), [3] (Lecture07.pdf p.29-30). \n",
      "A finite VC-dimension guarantees learnability, hence the VC-dimension characterizes PAC learnability.\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: How is VC dimension related to sample complexity?\n",
      "\n",
      "LLM-only:\n",
      " The VC (Vapnik-Chervonenkis) dimension is closely related to sample complexity in the context of statistical learning theory. \n",
      "\n",
      "The VC dimension of a hypothesis class is a measure of its complexity, representing the maximum number of points that can be shattered (i.e., classified in all possible ways) by the class. In other words, it's a measure of the capacity of the hypothesis class to fit any possible labeling of a set of points.\n",
      "\n",
      "Sample complexity, on the other hand, refers to the minimum number of samples required to learn a concept or hypothesis with a certain level of accuracy and confidence.\n",
      "\n",
      "The relationship between VC dimension and sample complexity is as follows:\n",
      "\n",
      "1. **Upper bound on sample complexity**: The VC dimension can be used to derive an upper bound on the sample complexity of a learning algorithm. Specifically, the VC dimension can be used to bound the number of samples required to achieve a certain level of accuracy and confidence. This is known as the \"VC bound\" or \"Vapnik-Chervonenkis bound\".\n",
      "2. **Trade-off between VC dimension and sample complexity**: A hypothesis class with a high VC dimension can fit a wide range of data, but it also requires a larger number of samples to achieve a certain level of accuracy and confidence. Conversely, a hypothesis class with a low VC dimension may not be able to fit complex data, but it requires fewer samples to achieve the same level of accuracy and confidence.\n",
      "3. **Consistency and convergence**: The\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (bm25):\n",
      " The VC dimension is related to sample complexity as it determines the number of samples required to achieve a certain level of accuracy with high confidence. In [1] (Lecture09.pdf p.14-16), it is shown that for the class of conjunctions of at most d Boolean literals, the VC dimension is d, and the sample complexity mH (Œµ,Œ¥) is bounded by a polynomial in 1/Œµ, 1/Œ¥, and d, which measures the complexity of the hypothesis class. This implies that the VC dimension plays a crucial role in determining the sample complexity of a learning algorithm. As stated in [1], \"the sample complexity mH (Œµ,Œ¥) is bounded by: ... a polynomial in 1/Œµ, 1/Œ¥ , d\", where d is the VC dimension of the hypothesis class.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (emb):\n",
      " The VC dimension is related to sample complexity as it determines (along with Œµ, Œ¥) the sample complexities of learning a class, providing both lower and upper bounds [3] (Lecture09.pdf p.6-7). A finite VC dimension guarantees learnability and characterizes PAC learnability [1] (Lecture06.pdf p.24-26), [2] (Lecture07.pdf p.29-30). Specifically, the sample complexity is bounded by constants that depend on the VC dimension d, as stated in the quantitative version of the fundamental theorem of statistical learning [3] (Lecture09.pdf p.6-7).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (hybrid):\n",
      " The VC dimension is related to sample complexity as it characterizes PAC learnability [1] (Lecture06.pdf p.24-26). A hypothesis class H with a finite VC dimension is PAC learnable, and the sample complexity mH(Œµ, Œ¥) is bounded by a polynomial in 1/Œµ, 1/Œ¥, and the VC dimension d [2] (Lecture09.pdf p.14-16). Specifically, for a hypothesis class H with VC dimension d, the sample complexity mH(Œµ, Œ¥) is polynomial in 1/Œµ, 1/Œ¥, and d [2] (Lecture09.pdf p.14-16). This is further supported by the fundamental theorem of learning, which states that if a hypothesis class H has a VC dimension d, then the sample complexity of PAC learning H satisfies a certain bound [3] (Lecture11.pdf p.25-26).\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: What is the Adam optimizer?\n",
      "\n",
      "LLM-only:\n",
      " **Adam Optimizer**\n",
      "====================\n",
      "\n",
      "The Adam optimizer is a popular stochastic gradient descent (SGD) algorithm used for training deep learning models. It was introduced by Kingma and Ba in 2014 and is widely used in the field of machine learning.\n",
      "\n",
      "**How Adam Works**\n",
      "-------------------\n",
      "\n",
      "Adam combines the benefits of two other popular optimizers:\n",
      "\n",
      "1. **Adagrad**: adapts the learning rate for each parameter based on the magnitude of the gradient.\n",
      "2. **RMSProp**: uses the magnitude of the recent gradients to normalize the update.\n",
      "\n",
      "The Adam optimizer calculates the update for each parameter as follows:\n",
      "\n",
      "1. Compute the gradient of the loss function with respect to the parameter.\n",
      "2. Calculate the first moment (mean) of the gradient.\n",
      "3. Calculate the second moment (variance) of the gradient.\n",
      "4. Update the parameter using the first and second moments.\n",
      "\n",
      "**Adam Update Rule**\n",
      "--------------------\n",
      "\n",
      "The Adam update rule is given by:\n",
      "\n",
      "`m_t = Œ≤1 * m_{t-1} + (1 - Œ≤1) * g_t`\n",
      "`v_t = Œ≤2 * v_{t-1} + (1 - Œ≤2) * g_t^2`\n",
      "`m_hat_t = m_t / (1 - Œ≤1^t)`\n",
      "`v_hat_t = v_t / (1 - Œ≤2^t)`\n",
      "`w_t = w_{t-1} - Œ± * m_hat_t / sqrt(v_hat_t + Œµ)`\n",
      "\n",
      "\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (bm25):\n",
      " Not in the provided course materials.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "Rate limited. Sleeping 8s...\n",
      "\n",
      "RAG (emb):\n",
      " Not in the provided course materials.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "Rate limited. Sleeping 8s...\n",
      "\n",
      "RAG (hybrid):\n",
      " Not in the provided course materials.\n",
      "\n",
      "==========================================================================================\n",
      "QUESTION: Explain transformers in deep learning.\n",
      "\n",
      "LLM-only:\n",
      " Transformers are a type of neural network architecture that have revolutionized the field of deep learning, particularly in natural language processing (NLP) and computer vision. Introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017, transformers have become a standard component in many state-of-the-art models.\n",
      "\n",
      "**What is a Transformer?**\n",
      "\n",
      "A transformer is a type of neural network that relies entirely on self-attention mechanisms to process input sequences, such as text or images. Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), transformers do not use recurrence or convolution to process input sequences. Instead, they use self-attention to weigh the importance of different input elements relative to each other.\n",
      "\n",
      "**Key Components of a Transformer:**\n",
      "\n",
      "1. **Self-Attention Mechanism**: This is the core component of a transformer. Self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is achieved through a set of learnable weights that compute the attention scores between different input elements.\n",
      "2. **Encoder-Decoder Architecture**: Transformers typically consist of an encoder and a decoder. The encoder takes in the input sequence and generates a continuous representation of the input. The decoder then generates the output sequence, one element at a time, based on the output of the encoder.\n",
      "3. **Multi-Head Attention**: Transformers use a multi-head attention mechanism, which allows the model to jointly attend\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "\n",
      "RAG (bm25):\n",
      " Not in the provided course materials. \n",
      "\n",
      "The context snippets provided do not mention transformers in deep learning. They discuss the basics of machine learning, PAC learning, and the differences between machine learning and statistics, but do not cover transformers [1] (Lecture01.pdf p.53-55), [2] (Lecture10.pdf p.8), [3] (Lecture10.pdf p.27-31), [4] (Lecture01.pdf p.46-52).\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "Rate limited. Sleeping 4s...\n",
      "\n",
      "RAG (emb):\n",
      " Not in the provided course materials.\n",
      "Rate limited. Sleeping 1s...\n",
      "Rate limited. Sleeping 2s...\n",
      "\n",
      "RAG (hybrid):\n",
      " Not in the provided course materials. \n",
      "\n",
      "The context snippets provided do not mention transformers in deep learning. They discuss the basics of machine learning, its applications, differences between machine learning and statistics, and some theoretical aspects of machine learning, but do not cover transformers [1], [2], [3].\n"
     ]
    }
   ],
   "source": [
    "EVAL_QUESTIONS = [\n",
    "    \"What is generalization error (true risk)?\",\n",
    "    \"Define gamma-weak learnability.\",\n",
    "    \"What is a hypothesis in statistical learning?\",\n",
    "    \"What is the main idea of AdaBoost?\",\n",
    "    \"What does hard-margin SVM optimize?\",\n",
    "    \"What is uniform convergence and why is it important?\",\n",
    "    \"What does the fundamental theorem of statistical learning state?\",\n",
    "    \"How is VC dimension related to sample complexity?\",\n",
    "    \"What is the Adam optimizer?\",\n",
    "    \"Explain transformers in deep learning.\"\n",
    "]\n",
    "\n",
    "def run_eval():\n",
    "    results = []\n",
    "\n",
    "    for q in EVAL_QUESTIONS:\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"QUESTION:\", q)\n",
    "\n",
    "        llm_ans = llm_only_answer(q)\n",
    "        print(\"\\nLLM-only:\\n\", llm_ans)\n",
    "\n",
    "        for mode in [\"bm25\", \"emb\", \"hybrid\"]:\n",
    "            out = rag_answer(q, mode=mode, k=5, type_filter=\"lecture\")\n",
    "            print(f\"\\nRAG ({mode}):\\n\", out[\"answer\"])\n",
    "\n",
    "            results.append({\n",
    "                \"question\": q,\n",
    "                \"mode\": mode,\n",
    "                \"answer\": out[\"answer\"],\n",
    "                \"retrieved\": out[\"retrieved\"]\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "eval_results = run_eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
